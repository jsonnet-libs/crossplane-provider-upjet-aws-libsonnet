{
  local d = (import 'doc-util/main.libsonnet'),
  '#':: d.pkg(name='pipe', url='', help='"Pipe is the Schema for the Pipes API."'),
  '#metadata':: d.obj(help='"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create."'),
  metadata: {
    '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotations(annotations): { metadata+: { annotations: annotations } },
    '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotationsMixin(annotations): { metadata+: { annotations+: annotations } },
    '#withClusterName':: d.fn(help='"The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request."', args=[d.arg(name='clusterName', type=d.T.string)]),
    withClusterName(clusterName): { metadata+: { clusterName: clusterName } },
    '#withCreationTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='creationTimestamp', type=d.T.string)]),
    withCreationTimestamp(creationTimestamp): { metadata+: { creationTimestamp: creationTimestamp } },
    '#withDeletionGracePeriodSeconds':: d.fn(help='"Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."', args=[d.arg(name='deletionGracePeriodSeconds', type=d.T.integer)]),
    withDeletionGracePeriodSeconds(deletionGracePeriodSeconds): { metadata+: { deletionGracePeriodSeconds: deletionGracePeriodSeconds } },
    '#withDeletionTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='deletionTimestamp', type=d.T.string)]),
    withDeletionTimestamp(deletionTimestamp): { metadata+: { deletionTimestamp: deletionTimestamp } },
    '#withFinalizers':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizers(finalizers): { metadata+: { finalizers: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withFinalizersMixin':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizersMixin(finalizers): { metadata+: { finalizers+: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withGenerateName':: d.fn(help='"GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.\\n\\nIf this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).\\n\\nApplied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"', args=[d.arg(name='generateName', type=d.T.string)]),
    withGenerateName(generateName): { metadata+: { generateName: generateName } },
    '#withGeneration':: d.fn(help='"A sequence number representing a specific generation of the desired state. Populated by the system. Read-only."', args=[d.arg(name='generation', type=d.T.integer)]),
    withGeneration(generation): { metadata+: { generation: generation } },
    '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
    withLabels(labels): { metadata+: { labels: labels } },
    '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
    withLabelsMixin(labels): { metadata+: { labels+: labels } },
    '#withName':: d.fn(help='"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names"', args=[d.arg(name='name', type=d.T.string)]),
    withName(name): { metadata+: { name: name } },
    '#withNamespace':: d.fn(help='"Namespace defines the space within which each name must be unique. An empty namespace is equivalent to the \\"default\\" namespace, but \\"default\\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.\\n\\nMust be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"', args=[d.arg(name='namespace', type=d.T.string)]),
    withNamespace(namespace): { metadata+: { namespace: namespace } },
    '#withOwnerReferences':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferences(ownerReferences): { metadata+: { ownerReferences: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withOwnerReferencesMixin':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferencesMixin(ownerReferences): { metadata+: { ownerReferences+: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withResourceVersion':: d.fn(help='"An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.\\n\\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"', args=[d.arg(name='resourceVersion', type=d.T.string)]),
    withResourceVersion(resourceVersion): { metadata+: { resourceVersion: resourceVersion } },
    '#withSelfLink':: d.fn(help='"SelfLink is a URL representing this object. Populated by the system. Read-only.\\n\\nDEPRECATED Kubernetes will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."', args=[d.arg(name='selfLink', type=d.T.string)]),
    withSelfLink(selfLink): { metadata+: { selfLink: selfLink } },
    '#withUid':: d.fn(help='"UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.\\n\\nPopulated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"', args=[d.arg(name='uid', type=d.T.string)]),
    withUid(uid): { metadata+: { uid: uid } },
  },
  '#new':: d.fn(help='new returns an instance of Pipe', args=[d.arg(name='name', type=d.T.string)]),
  new(name): {
    apiVersion: 'pipes.aws.m.upbound.io/v1beta1',
    kind: 'Pipe',
  } + self.metadata.withName(name=name),
  '#spec':: d.obj(help='"PipeSpec defines the desired state of Pipe"'),
  spec: {
    '#forProvider':: d.obj(help=''),
    forProvider: {
      '#enrichmentParameters':: d.obj(help='"Parameters to configure enrichment for your pipe. Detailed below."'),
      enrichmentParameters: {
        '#httpParameters':: d.obj(help="\"Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.\""),
        httpParameters: {
          '#withHeaderParameters':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParameters(headerParameters): { spec+: { forProvider+: { enrichmentParameters+: { httpParameters+: { headerParameters: headerParameters } } } } },
          '#withHeaderParametersMixin':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParametersMixin(headerParameters): { spec+: { forProvider+: { enrichmentParameters+: { httpParameters+: { headerParameters+: headerParameters } } } } },
          '#withPathParameterValues':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValues(pathParameterValues): { spec+: { forProvider+: { enrichmentParameters+: { httpParameters+: { pathParameterValues: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withPathParameterValuesMixin':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValuesMixin(pathParameterValues): { spec+: { forProvider+: { enrichmentParameters+: { httpParameters+: { pathParameterValues+: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withQueryStringParameters':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParameters(queryStringParameters): { spec+: { forProvider+: { enrichmentParameters+: { httpParameters+: { queryStringParameters: queryStringParameters } } } } },
          '#withQueryStringParametersMixin':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParametersMixin(queryStringParameters): { spec+: { forProvider+: { enrichmentParameters+: { httpParameters+: { queryStringParameters+: queryStringParameters } } } } },
        },
        '#withInputTemplate':: d.fn(help='"Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters."', args=[d.arg(name='inputTemplate', type=d.T.string)]),
        withInputTemplate(inputTemplate): { spec+: { forProvider+: { enrichmentParameters+: { inputTemplate: inputTemplate } } } },
      },
      '#enrichmentRef':: d.obj(help='"Reference to a APIDestination in cloudwatchevents to populate enrichment."'),
      enrichmentRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { enrichmentRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { enrichmentRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { forProvider+: { enrichmentRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { enrichmentRef+: { namespace: namespace } } } },
      },
      '#enrichmentSelector':: d.obj(help='"Selector for a APIDestination in cloudwatchevents to populate enrichment."'),
      enrichmentSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { enrichmentSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { enrichmentSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { forProvider+: { enrichmentSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { forProvider+: { enrichmentSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { forProvider+: { enrichmentSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { enrichmentSelector+: { namespace: namespace } } } },
      },
      '#logConfiguration':: d.obj(help='"Logging configuration settings for the pipe. Detailed below."'),
      logConfiguration: {
        '#cloudwatchLogsLogDestination':: d.obj(help='"Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below."'),
        cloudwatchLogsLogDestination: {
          '#logGroupArnRef':: d.obj(help='"Reference to a Group in cloudwatchlogs to populate logGroupArn."'),
          logGroupArnRef: {
            '#policy':: d.obj(help='"Policies for referencing."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { policy+: { resolution: resolution } } } } } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { policy+: { resolve: resolve } } } } } } },
            },
            '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { name: name } } } } } },
            '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
            withNamespace(namespace): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { namespace: namespace } } } } } },
          },
          '#logGroupArnSelector':: d.obj(help='"Selector for a Group in cloudwatchlogs to populate logGroupArn."'),
          logGroupArnSelector: {
            '#policy':: d.obj(help='"Policies for selection."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { policy+: { resolution: resolution } } } } } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { policy+: { resolve: resolve } } } } } } },
            },
            '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
            withMatchControllerRef(matchControllerRef): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { matchControllerRef: matchControllerRef } } } } } },
            '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { matchLabels: matchLabels } } } } } },
            '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { matchLabels+: matchLabels } } } } } },
            '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
            withNamespace(namespace): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { namespace: namespace } } } } } },
          },
          '#withLogGroupArn':: d.fn(help='"Amazon Web Services Resource Name (ARN) for the CloudWatch log group to which EventBridge sends the log records."', args=[d.arg(name='logGroupArn', type=d.T.string)]),
          withLogGroupArn(logGroupArn): { spec+: { forProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArn: logGroupArn } } } } },
        },
        '#firehoseLogDestination':: d.obj(help='"Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below."'),
        firehoseLogDestination: {
          '#withDeliveryStreamArn':: d.fn(help='"Amazon Resource Name (ARN) of the Kinesis Data Firehose delivery stream to which EventBridge delivers the pipe log records."', args=[d.arg(name='deliveryStreamArn', type=d.T.string)]),
          withDeliveryStreamArn(deliveryStreamArn): { spec+: { forProvider+: { logConfiguration+: { firehoseLogDestination+: { deliveryStreamArn: deliveryStreamArn } } } } },
        },
        '#s3LogDestination':: d.obj(help='"Amazon S3 logging configuration settings for the pipe. Detailed below."'),
        s3LogDestination: {
          '#withBucketName':: d.fn(help='"Name of the Amazon S3 bucket to which EventBridge delivers the log records for the pipe."', args=[d.arg(name='bucketName', type=d.T.string)]),
          withBucketName(bucketName): { spec+: { forProvider+: { logConfiguration+: { s3LogDestination+: { bucketName: bucketName } } } } },
          '#withBucketOwner':: d.fn(help='"Amazon Web Services account that owns the Amazon S3 bucket to which EventBridge delivers the log records for the pipe."', args=[d.arg(name='bucketOwner', type=d.T.string)]),
          withBucketOwner(bucketOwner): { spec+: { forProvider+: { logConfiguration+: { s3LogDestination+: { bucketOwner: bucketOwner } } } } },
          '#withOutputFormat':: d.fn(help='"EventBridge format for the log records. Valid values json, plain and w3c."', args=[d.arg(name='outputFormat', type=d.T.string)]),
          withOutputFormat(outputFormat): { spec+: { forProvider+: { logConfiguration+: { s3LogDestination+: { outputFormat: outputFormat } } } } },
          '#withPrefix':: d.fn(help='"Prefix text with which to begin Amazon S3 log object names."', args=[d.arg(name='prefix', type=d.T.string)]),
          withPrefix(prefix): { spec+: { forProvider+: { logConfiguration+: { s3LogDestination+: { prefix: prefix } } } } },
        },
        '#withIncludeExecutionData':: d.fn(help='"String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL."', args=[d.arg(name='includeExecutionData', type=d.T.array)]),
        withIncludeExecutionData(includeExecutionData): { spec+: { forProvider+: { logConfiguration+: { includeExecutionData: if std.isArray(v=includeExecutionData) then includeExecutionData else [includeExecutionData] } } } },
        '#withIncludeExecutionDataMixin':: d.fn(help='"String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='includeExecutionData', type=d.T.array)]),
        withIncludeExecutionDataMixin(includeExecutionData): { spec+: { forProvider+: { logConfiguration+: { includeExecutionData+: if std.isArray(v=includeExecutionData) then includeExecutionData else [includeExecutionData] } } } },
        '#withLevel':: d.fn(help='"The level of logging detail to include. Valid values OFF, ERROR, INFO and TRACE."', args=[d.arg(name='level', type=d.T.string)]),
        withLevel(level): { spec+: { forProvider+: { logConfiguration+: { level: level } } } },
      },
      '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
      roleArnRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { roleArnRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { roleArnRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { forProvider+: { roleArnRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { roleArnRef+: { namespace: namespace } } } },
      },
      '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
      roleArnSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { roleArnSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { roleArnSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { forProvider+: { roleArnSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { forProvider+: { roleArnSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { forProvider+: { roleArnSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { roleArnSelector+: { namespace: namespace } } } },
      },
      '#sourceParameters':: d.obj(help='"Parameters to configure a source for the pipe. Detailed below."'),
      sourceParameters: {
        '#activemqBrokerParameters':: d.obj(help='"The parameters for using an Active MQ broker as a source. Detailed below."'),
        activemqBrokerParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withBasicAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the basic auth credentials."', args=[d.arg(name='basicAuth', type=d.T.string)]),
            withBasicAuth(basicAuth): { spec+: { forProvider+: { sourceParameters+: { activemqBrokerParameters+: { credentials+: { basicAuth: basicAuth } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { activemqBrokerParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { activemqBrokerParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withQueueName':: d.fn(help='"The name of the destination queue to consume. Maximum length of 1000."', args=[d.arg(name='queueName', type=d.T.string)]),
          withQueueName(queueName): { spec+: { forProvider+: { sourceParameters+: { activemqBrokerParameters+: { queueName: queueName } } } } },
        },
        '#dynamodbStreamParameters':: d.obj(help='"The parameters for using a DynamoDB stream as a source.  Detailed below."'),
        dynamodbStreamParameters: {
          '#deadLetterConfig':: d.obj(help='"Define the target queue to send dead-letter queue events to. Detailed below."'),
          deadLetterConfig: {
            '#withArn':: d.fn(help='"The ARN of the Amazon SQS queue specified as the target for the dead-letter queue."', args=[d.arg(name='arn', type=d.T.string)]),
            withArn(arn): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { deadLetterConfig+: { arn: arn } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withMaximumRecordAgeInSeconds':: d.fn(help='"Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800."', args=[d.arg(name='maximumRecordAgeInSeconds', type=d.T.number)]),
          withMaximumRecordAgeInSeconds(maximumRecordAgeInSeconds): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { maximumRecordAgeInSeconds: maximumRecordAgeInSeconds } } } } },
          '#withMaximumRetryAttempts':: d.fn(help='"Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000."', args=[d.arg(name='maximumRetryAttempts', type=d.T.number)]),
          withMaximumRetryAttempts(maximumRetryAttempts): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { maximumRetryAttempts: maximumRetryAttempts } } } } },
          '#withOnPartialBatchItemFailure':: d.fn(help='"Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT."', args=[d.arg(name='onPartialBatchItemFailure', type=d.T.string)]),
          withOnPartialBatchItemFailure(onPartialBatchItemFailure): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { onPartialBatchItemFailure: onPartialBatchItemFailure } } } } },
          '#withParallelizationFactor':: d.fn(help='"The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10."', args=[d.arg(name='parallelizationFactor', type=d.T.number)]),
          withParallelizationFactor(parallelizationFactor): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { parallelizationFactor: parallelizationFactor } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { forProvider+: { sourceParameters+: { dynamodbStreamParameters+: { startingPosition: startingPosition } } } } },
        },
        '#filterCriteria':: d.obj(help='"The collection of event patterns used to filter events. Detailed below."'),
        filterCriteria: {
          '#filter':: d.obj(help='"An array of up to 5 event patterns. Detailed below."'),
          filter: {
            '#withPattern':: d.fn(help='"The event pattern. At most 4096 characters."', args=[d.arg(name='pattern', type=d.T.string)]),
            withPattern(pattern): { pattern: pattern },
          },
          '#withFilter':: d.fn(help='"An array of up to 5 event patterns. Detailed below."', args=[d.arg(name='filter', type=d.T.array)]),
          withFilter(filter): { spec+: { forProvider+: { sourceParameters+: { filterCriteria+: { filter: if std.isArray(v=filter) then filter else [filter] } } } } },
          '#withFilterMixin':: d.fn(help='"An array of up to 5 event patterns. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='filter', type=d.T.array)]),
          withFilterMixin(filter): { spec+: { forProvider+: { sourceParameters+: { filterCriteria+: { filter+: if std.isArray(v=filter) then filter else [filter] } } } } },
        },
        '#kinesisStreamParameters':: d.obj(help='"The parameters for using a Kinesis stream as a source. Detailed below."'),
        kinesisStreamParameters: {
          '#deadLetterConfig':: d.obj(help='"Define the target queue to send dead-letter queue events to. Detailed below."'),
          deadLetterConfig: {
            '#withArn':: d.fn(help='"The ARN of the Amazon SQS queue specified as the target for the dead-letter queue."', args=[d.arg(name='arn', type=d.T.string)]),
            withArn(arn): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { deadLetterConfig+: { arn: arn } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withMaximumRecordAgeInSeconds':: d.fn(help='"Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800."', args=[d.arg(name='maximumRecordAgeInSeconds', type=d.T.number)]),
          withMaximumRecordAgeInSeconds(maximumRecordAgeInSeconds): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { maximumRecordAgeInSeconds: maximumRecordAgeInSeconds } } } } },
          '#withMaximumRetryAttempts':: d.fn(help='"Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000."', args=[d.arg(name='maximumRetryAttempts', type=d.T.number)]),
          withMaximumRetryAttempts(maximumRetryAttempts): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { maximumRetryAttempts: maximumRetryAttempts } } } } },
          '#withOnPartialBatchItemFailure':: d.fn(help='"Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT."', args=[d.arg(name='onPartialBatchItemFailure', type=d.T.string)]),
          withOnPartialBatchItemFailure(onPartialBatchItemFailure): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { onPartialBatchItemFailure: onPartialBatchItemFailure } } } } },
          '#withParallelizationFactor':: d.fn(help='"The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10."', args=[d.arg(name='parallelizationFactor', type=d.T.number)]),
          withParallelizationFactor(parallelizationFactor): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { parallelizationFactor: parallelizationFactor } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST, AT_TIMESTAMP."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { startingPosition: startingPosition } } } } },
          '#withStartingPositionTimestamp':: d.fn(help='"With StartingPosition set to AT_TIMESTAMP, the time from which to start reading, in Unix time seconds."', args=[d.arg(name='startingPositionTimestamp', type=d.T.string)]),
          withStartingPositionTimestamp(startingPositionTimestamp): { spec+: { forProvider+: { sourceParameters+: { kinesisStreamParameters+: { startingPositionTimestamp: startingPositionTimestamp } } } } },
        },
        '#managedStreamingKafkaParameters':: d.obj(help='"The parameters for using an MSK stream as a source. Detailed below."'),
        managedStreamingKafkaParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withClientCertificateTlsAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='clientCertificateTlsAuth', type=d.T.string)]),
            withClientCertificateTlsAuth(clientCertificateTlsAuth): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { credentials+: { clientCertificateTlsAuth: clientCertificateTlsAuth } } } } } },
            '#withSaslScram512Auth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='saslScram512Auth', type=d.T.string)]),
            withSaslScram512Auth(saslScram512Auth): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { credentials+: { saslScram512Auth: saslScram512Auth } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { batchSize: batchSize } } } } },
          '#withConsumerGroupId':: d.fn(help='"The name of the destination queue to consume. Maximum value of 200."', args=[d.arg(name='consumerGroupId', type=d.T.string)]),
          withConsumerGroupId(consumerGroupId): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { consumerGroupId: consumerGroupId } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { startingPosition: startingPosition } } } } },
          '#withTopicName':: d.fn(help='"The name of the topic that the pipe will read from. Maximum length of 249."', args=[d.arg(name='topicName', type=d.T.string)]),
          withTopicName(topicName): { spec+: { forProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { topicName: topicName } } } } },
        },
        '#rabbitmqBrokerParameters':: d.obj(help='"The parameters for using a Rabbit MQ broker as a source. Detailed below."'),
        rabbitmqBrokerParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withBasicAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='basicAuth', type=d.T.string)]),
            withBasicAuth(basicAuth): { spec+: { forProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { credentials+: { basicAuth: basicAuth } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withQueueName':: d.fn(help='"The name of the destination queue to consume. Maximum length of 1000."', args=[d.arg(name='queueName', type=d.T.string)]),
          withQueueName(queueName): { spec+: { forProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { queueName: queueName } } } } },
          '#withVirtualHost':: d.fn(help='"The name of the virtual host associated with the source broker. Maximum length of 200."', args=[d.arg(name='virtualHost', type=d.T.string)]),
          withVirtualHost(virtualHost): { spec+: { forProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { virtualHost: virtualHost } } } } },
        },
        '#selfManagedKafkaParameters':: d.obj(help='"The parameters for using a self-managed Apache Kafka stream as a source. Detailed below."'),
        selfManagedKafkaParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withBasicAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='basicAuth', type=d.T.string)]),
            withBasicAuth(basicAuth): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { basicAuth: basicAuth } } } } } },
            '#withClientCertificateTlsAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='clientCertificateTlsAuth', type=d.T.string)]),
            withClientCertificateTlsAuth(clientCertificateTlsAuth): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { clientCertificateTlsAuth: clientCertificateTlsAuth } } } } } },
            '#withSaslScram256Auth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='saslScram256Auth', type=d.T.string)]),
            withSaslScram256Auth(saslScram256Auth): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { saslScram256Auth: saslScram256Auth } } } } } },
            '#withSaslScram512Auth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='saslScram512Auth', type=d.T.string)]),
            withSaslScram512Auth(saslScram512Auth): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { saslScram512Auth: saslScram512Auth } } } } } },
          },
          '#vpc':: d.obj(help='"This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below."'),
          vpc: {
            '#withSecurityGroups':: d.fn(help='"List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."', args=[d.arg(name='securityGroups', type=d.T.array)]),
            withSecurityGroups(securityGroups): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { securityGroups: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } },
            '#withSecurityGroupsMixin':: d.fn(help='"List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='securityGroups', type=d.T.array)]),
            withSecurityGroupsMixin(securityGroups): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { securityGroups+: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } },
            '#withSubnets':: d.fn(help='"List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets."', args=[d.arg(name='subnets', type=d.T.array)]),
            withSubnets(subnets): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { subnets: if std.isArray(v=subnets) then subnets else [subnets] } } } } } },
            '#withSubnetsMixin':: d.fn(help='"List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='subnets', type=d.T.array)]),
            withSubnetsMixin(subnets): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { subnets+: if std.isArray(v=subnets) then subnets else [subnets] } } } } } },
          },
          '#withAdditionalBootstrapServers':: d.fn(help='"An array of server URLs. Maximum number of 2 items, each of maximum length 300."', args=[d.arg(name='additionalBootstrapServers', type=d.T.array)]),
          withAdditionalBootstrapServers(additionalBootstrapServers): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { additionalBootstrapServers: if std.isArray(v=additionalBootstrapServers) then additionalBootstrapServers else [additionalBootstrapServers] } } } } },
          '#withAdditionalBootstrapServersMixin':: d.fn(help='"An array of server URLs. Maximum number of 2 items, each of maximum length 300."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='additionalBootstrapServers', type=d.T.array)]),
          withAdditionalBootstrapServersMixin(additionalBootstrapServers): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { additionalBootstrapServers+: if std.isArray(v=additionalBootstrapServers) then additionalBootstrapServers else [additionalBootstrapServers] } } } } },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { batchSize: batchSize } } } } },
          '#withConsumerGroupId':: d.fn(help='"The name of the destination queue to consume. Maximum value of 200."', args=[d.arg(name='consumerGroupId', type=d.T.string)]),
          withConsumerGroupId(consumerGroupId): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { consumerGroupId: consumerGroupId } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withServerRootCaCertificate':: d.fn(help='"The ARN of the Secrets Manager secret used for certification."', args=[d.arg(name='serverRootCaCertificate', type=d.T.string)]),
          withServerRootCaCertificate(serverRootCaCertificate): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { serverRootCaCertificate: serverRootCaCertificate } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { startingPosition: startingPosition } } } } },
          '#withTopicName':: d.fn(help='"The name of the topic that the pipe will read from. Maximum length of 249."', args=[d.arg(name='topicName', type=d.T.string)]),
          withTopicName(topicName): { spec+: { forProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { topicName: topicName } } } } },
        },
        '#sqsQueueParameters':: d.obj(help='"The parameters for using a Amazon SQS stream as a source. Detailed below."'),
        sqsQueueParameters: {
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { forProvider+: { sourceParameters+: { sqsQueueParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { forProvider+: { sourceParameters+: { sqsQueueParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
        },
      },
      '#sourceRef':: d.obj(help='"Reference to a Queue in sqs to populate source."'),
      sourceRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { sourceRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { sourceRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { forProvider+: { sourceRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { sourceRef+: { namespace: namespace } } } },
      },
      '#sourceSelector':: d.obj(help='"Selector for a Queue in sqs to populate source."'),
      sourceSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { sourceSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { sourceSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { forProvider+: { sourceSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { forProvider+: { sourceSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { forProvider+: { sourceSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { sourceSelector+: { namespace: namespace } } } },
      },
      '#targetParameters':: d.obj(help='"Parameters to configure a target for your pipe. Detailed below."'),
      targetParameters: {
        '#batchJobParameters':: d.obj(help='"The parameters for using an AWS Batch job as a target. Detailed below."'),
        batchJobParameters: {
          '#arrayProperties':: d.obj(help='"The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below."'),
          arrayProperties: {
            '#withSize':: d.fn(help='"The size of the array, if this is an array batch job. Minimum value of 2. Maximum value of 10,000."', args=[d.arg(name='size', type=d.T.number)]),
            withSize(size): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { arrayProperties+: { size: size } } } } } },
          },
          '#containerOverrides':: d.obj(help='"The overrides that are sent to a container. Detailed below."'),
          containerOverrides: {
            '#environment':: d.obj(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with \\" AWS Batch \\". This naming convention is reserved for variables that AWS Batch sets. Detailed below."'),
            environment: {
              '#withName':: d.fn(help='"The name of the key-value pair. For environment variables, this is the name of the environment variable."', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { name: name },
              '#withValue':: d.fn(help='"The value of the key-value pair. For environment variables, this is the value of the environment variable."', args=[d.arg(name='value', type=d.T.string)]),
              withValue(value): { value: value },
            },
            '#resourceRequirement':: d.obj(help='"The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below."'),
            resourceRequirement: {
              '#withType':: d.fn(help='"The type of resource to assign to a container. The supported resources include GPU, MEMORY, and VCPU."', args=[d.arg(name='type', type=d.T.string)]),
              withType(type): { type: type },
              '#withValue':: d.fn(help='"The quantity of the specified resource to reserve for the container. The values vary based on the type specified."', args=[d.arg(name='value', type=d.T.string)]),
              withValue(value): { value: value },
            },
            '#withCommand':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition."', args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
            '#withEnvironment':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with \\" AWS Batch \\". This naming convention is reserved for variables that AWS Batch sets. Detailed below."', args=[d.arg(name='environment', type=d.T.array)]),
            withEnvironment(environment): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { environment: if std.isArray(v=environment) then environment else [environment] } } } } } },
            '#withEnvironmentMixin':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with \\" AWS Batch \\". This naming convention is reserved for variables that AWS Batch sets. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='environment', type=d.T.array)]),
            withEnvironmentMixin(environment): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { environment+: if std.isArray(v=environment) then environment else [environment] } } } } } },
            '#withInstanceType':: d.fn(help="\"The instance type to use for a multi-node parallel job. This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.\"", args=[d.arg(name='instanceType', type=d.T.string)]),
            withInstanceType(instanceType): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { instanceType: instanceType } } } } } },
            '#withResourceRequirement':: d.fn(help='"The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below."', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
            withResourceRequirement(resourceRequirement): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { resourceRequirement: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] } } } } } },
            '#withResourceRequirementMixin':: d.fn(help='"The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
            withResourceRequirementMixin(resourceRequirement): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { resourceRequirement+: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] } } } } } },
          },
          '#dependsOn':: d.obj(help='"A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below."'),
          dependsOn: {
            '#withJobId':: d.fn(help="\"The job ID of the AWS Batch job that's associated with this dependency.\"", args=[d.arg(name='jobId', type=d.T.string)]),
            withJobId(jobId): { jobId: jobId },
            '#withType':: d.fn(help='"The type of the job dependency. Valid Values: N_TO_N, SEQUENTIAL."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#retryStrategy':: d.obj(help='"The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below."'),
          retryStrategy: {
            '#withAttempts':: d.fn(help='"The number of times to move a job to the RUNNABLE status. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Maximum value of 10."', args=[d.arg(name='attempts', type=d.T.number)]),
            withAttempts(attempts): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { retryStrategy+: { attempts: attempts } } } } } },
          },
          '#withDependsOn':: d.fn(help='"A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below."', args=[d.arg(name='dependsOn', type=d.T.array)]),
          withDependsOn(dependsOn): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { dependsOn: if std.isArray(v=dependsOn) then dependsOn else [dependsOn] } } } } },
          '#withDependsOnMixin':: d.fn(help='"A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='dependsOn', type=d.T.array)]),
          withDependsOnMixin(dependsOn): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { dependsOn+: if std.isArray(v=dependsOn) then dependsOn else [dependsOn] } } } } },
          '#withJobDefinition':: d.fn(help='"The job definition used by this job. This value can be one of name, name:revision, or the Amazon Resource Name (ARN) for the job definition. If name is specified without a revision then the latest active revision is used."', args=[d.arg(name='jobDefinition', type=d.T.string)]),
          withJobDefinition(jobDefinition): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { jobDefinition: jobDefinition } } } } },
          '#withJobName':: d.fn(help='"The name of the job. It can be up to 128 letters long."', args=[d.arg(name='jobName', type=d.T.string)]),
          withJobName(jobName): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { jobName: jobName } } } } },
          '#withParameters':: d.fn(help='"Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below."', args=[d.arg(name='parameters', type=d.T.object)]),
          withParameters(parameters): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { parameters: parameters } } } } },
          '#withParametersMixin':: d.fn(help='"Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.object)]),
          withParametersMixin(parameters): { spec+: { forProvider+: { targetParameters+: { batchJobParameters+: { parameters+: parameters } } } } },
        },
        '#cloudwatchLogsParameters':: d.obj(help='"The parameters for using an CloudWatch Logs log stream as a target. Detailed below."'),
        cloudwatchLogsParameters: {
          '#withLogStreamName':: d.fn(help='"The name of the log stream."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { spec+: { forProvider+: { targetParameters+: { cloudwatchLogsParameters+: { logStreamName: logStreamName } } } } },
          '#withTimestamp':: d.fn(help='"The time the event occurred, expressed as the number of milliseconds after Jan 1, 1970 00:00:00 UTC. This is the JSON path to the field in the event e.g. $.detail.timestamp"', args=[d.arg(name='timestamp', type=d.T.string)]),
          withTimestamp(timestamp): { spec+: { forProvider+: { targetParameters+: { cloudwatchLogsParameters+: { timestamp: timestamp } } } } },
        },
        '#ecsTaskParameters':: d.obj(help='"The parameters for using an Amazon ECS task as a target. Detailed below."'),
        ecsTaskParameters: {
          '#capacityProviderStrategy':: d.obj(help='"List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below."'),
          capacityProviderStrategy: {
            '#withBase':: d.fn(help='"The base value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a base defined. If no value is specified, the default value of 0 is used. Maximum value of 100,000."', args=[d.arg(name='base', type=d.T.number)]),
            withBase(base): { base: base },
            '#withCapacityProvider':: d.fn(help='"The short name of the capacity provider. Maximum value of 255."', args=[d.arg(name='capacityProvider', type=d.T.string)]),
            withCapacityProvider(capacityProvider): { capacityProvider: capacityProvider },
            '#withWeight':: d.fn(help='"The weight value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The weight value is taken into consideration after the base value, if defined, is satisfied. Maximum value of 1,000."', args=[d.arg(name='weight', type=d.T.number)]),
            withWeight(weight): { weight: weight },
          },
          '#networkConfiguration':: d.obj(help='"Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below."'),
          networkConfiguration: {
            '#awsVpcConfiguration':: d.obj(help='"Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below."'),
            awsVpcConfiguration: {
              '#withAssignPublicIp':: d.fn(help="\"Specifies whether the task's elastic network interface receives a public IP address. You can specify ENABLED only when LaunchType in EcsParameters is set to FARGATE. Valid Values: ENABLED, DISABLED.\"", args=[d.arg(name='assignPublicIp', type=d.T.string)]),
              withAssignPublicIp(assignPublicIp): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { assignPublicIp: assignPublicIp } } } } } } },
              '#withSecurityGroups':: d.fn(help='"Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."', args=[d.arg(name='securityGroups', type=d.T.array)]),
              withSecurityGroups(securityGroups): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { securityGroups: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } } },
              '#withSecurityGroupsMixin':: d.fn(help='"Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='securityGroups', type=d.T.array)]),
              withSecurityGroupsMixin(securityGroups): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { securityGroups+: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } } },
              '#withSubnets':: d.fn(help='"Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets."', args=[d.arg(name='subnets', type=d.T.array)]),
              withSubnets(subnets): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { subnets: if std.isArray(v=subnets) then subnets else [subnets] } } } } } } },
              '#withSubnetsMixin':: d.fn(help='"Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='subnets', type=d.T.array)]),
              withSubnetsMixin(subnets): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { subnets+: if std.isArray(v=subnets) then subnets else [subnets] } } } } } } },
            },
          },
          '#overrides':: d.obj(help='"The overrides that are associated with a task. Detailed below."'),
          overrides: {
            '#containerOverride':: d.obj(help='"One or more container overrides that are sent to a task. Detailed below."'),
            containerOverride: {
              '#environment':: d.obj(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below."'),
              environment: {
                '#withName':: d.fn(help='"The name of the key-value pair. For environment variables, this is the name of the environment variable."', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { name: name },
                '#withValue':: d.fn(help='"The value of the key-value pair. For environment variables, this is the value of the environment variable."', args=[d.arg(name='value', type=d.T.string)]),
                withValue(value): { value: value },
              },
              '#environmentFile':: d.obj(help='"A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below."'),
              environmentFile: {
                '#withType':: d.fn(help='"The file type to use. The only supported value is s3."', args=[d.arg(name='type', type=d.T.string)]),
                withType(type): { type: type },
                '#withValue':: d.fn(help='"The Amazon Resource Name (ARN) of the Amazon S3 object containing the environment variable file."', args=[d.arg(name='value', type=d.T.string)]),
                withValue(value): { value: value },
              },
              '#resourceRequirement':: d.obj(help='"The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below."'),
              resourceRequirement: {
                '#withType':: d.fn(help='"The type of resource to assign to a container. The supported values are GPU or InferenceAccelerator."', args=[d.arg(name='type', type=d.T.string)]),
                withType(type): { type: type },
                '#withValue':: d.fn(help="\"The value for the specified resource type. If the GPU type is used, the value is the number of physical GPUs the Amazon ECS container agent reserves for the container. The number of GPUs that's reserved for all containers in a task can't exceed the number of available GPUs on the container instance that the task is launched on. If the InferenceAccelerator type is used, the value matches the deviceName for an InferenceAccelerator specified in a task definition.\"", args=[d.arg(name='value', type=d.T.string)]),
                withValue(value): { value: value },
              },
              '#withCommand':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name."', args=[d.arg(name='command', type=d.T.array)]),
              withCommand(command): { command: if std.isArray(v=command) then command else [command] },
              '#withCommandMixin':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='command', type=d.T.array)]),
              withCommandMixin(command): { command+: if std.isArray(v=command) then command else [command] },
              '#withCpu':: d.fn(help='"The number of cpu units reserved for the container, instead of the default value from the task definition. You must also specify a container name."', args=[d.arg(name='cpu', type=d.T.number)]),
              withCpu(cpu): { cpu: cpu },
              '#withEnvironment':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below."', args=[d.arg(name='environment', type=d.T.array)]),
              withEnvironment(environment): { environment: if std.isArray(v=environment) then environment else [environment] },
              '#withEnvironmentFile':: d.fn(help='"A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below."', args=[d.arg(name='environmentFile', type=d.T.array)]),
              withEnvironmentFile(environmentFile): { environmentFile: if std.isArray(v=environmentFile) then environmentFile else [environmentFile] },
              '#withEnvironmentFileMixin':: d.fn(help='"A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='environmentFile', type=d.T.array)]),
              withEnvironmentFileMixin(environmentFile): { environmentFile+: if std.isArray(v=environmentFile) then environmentFile else [environmentFile] },
              '#withEnvironmentMixin':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='environment', type=d.T.array)]),
              withEnvironmentMixin(environment): { environment+: if std.isArray(v=environment) then environment else [environment] },
              '#withMemory':: d.fn(help='"The hard limit (in MiB) of memory to present to the container, instead of the default value from the task definition. If your container attempts to exceed the memory specified here, the container is killed. You must also specify a container name."', args=[d.arg(name='memory', type=d.T.number)]),
              withMemory(memory): { memory: memory },
              '#withMemoryReservation':: d.fn(help='"The soft limit (in MiB) of memory to reserve for the container, instead of the default value from the task definition. You must also specify a container name."', args=[d.arg(name='memoryReservation', type=d.T.number)]),
              withMemoryReservation(memoryReservation): { memoryReservation: memoryReservation },
              '#withName':: d.fn(help='"The name of the container that receives the override. This parameter is required if any override is specified."', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { name: name },
              '#withResourceRequirement':: d.fn(help='"The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below."', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
              withResourceRequirement(resourceRequirement): { resourceRequirement: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] },
              '#withResourceRequirementMixin':: d.fn(help='"The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
              withResourceRequirementMixin(resourceRequirement): { resourceRequirement+: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] },
            },
            '#ephemeralStorage':: d.obj(help='"The ephemeral storage setting override for the task.  Detailed below."'),
            ephemeralStorage: {
              '#withSizeInGib':: d.fn(help='"The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is 21 GiB and the maximum supported value is 200 GiB."', args=[d.arg(name='sizeInGib', type=d.T.number)]),
              withSizeInGib(sizeInGib): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { ephemeralStorage+: { sizeInGib: sizeInGib } } } } } } },
            },
            '#inferenceAcceleratorOverride':: d.obj(help='"List of Elastic Inference accelerator overrides for the task. Detailed below."'),
            inferenceAcceleratorOverride: {
              '#withDeviceName':: d.fn(help='"The Elastic Inference accelerator device name to override for the task. This parameter must match a deviceName specified in the task definition."', args=[d.arg(name='deviceName', type=d.T.string)]),
              withDeviceName(deviceName): { deviceName: deviceName },
              '#withDeviceType':: d.fn(help='"The Elastic Inference accelerator type to use."', args=[d.arg(name='deviceType', type=d.T.string)]),
              withDeviceType(deviceType): { deviceType: deviceType },
            },
            '#withContainerOverride':: d.fn(help='"One or more container overrides that are sent to a task. Detailed below."', args=[d.arg(name='containerOverride', type=d.T.array)]),
            withContainerOverride(containerOverride): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { containerOverride: if std.isArray(v=containerOverride) then containerOverride else [containerOverride] } } } } } },
            '#withContainerOverrideMixin':: d.fn(help='"One or more container overrides that are sent to a task. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='containerOverride', type=d.T.array)]),
            withContainerOverrideMixin(containerOverride): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { containerOverride+: if std.isArray(v=containerOverride) then containerOverride else [containerOverride] } } } } } },
            '#withCpu':: d.fn(help='"The cpu override for the task."', args=[d.arg(name='cpu', type=d.T.string)]),
            withCpu(cpu): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { cpu: cpu } } } } } },
            '#withExecutionRoleArn':: d.fn(help='"The Amazon Resource Name (ARN) of the task execution IAM role override for the task."', args=[d.arg(name='executionRoleArn', type=d.T.string)]),
            withExecutionRoleArn(executionRoleArn): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { executionRoleArn: executionRoleArn } } } } } },
            '#withInferenceAcceleratorOverride':: d.fn(help='"List of Elastic Inference accelerator overrides for the task. Detailed below."', args=[d.arg(name='inferenceAcceleratorOverride', type=d.T.array)]),
            withInferenceAcceleratorOverride(inferenceAcceleratorOverride): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { inferenceAcceleratorOverride: if std.isArray(v=inferenceAcceleratorOverride) then inferenceAcceleratorOverride else [inferenceAcceleratorOverride] } } } } } },
            '#withInferenceAcceleratorOverrideMixin':: d.fn(help='"List of Elastic Inference accelerator overrides for the task. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='inferenceAcceleratorOverride', type=d.T.array)]),
            withInferenceAcceleratorOverrideMixin(inferenceAcceleratorOverride): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { inferenceAcceleratorOverride+: if std.isArray(v=inferenceAcceleratorOverride) then inferenceAcceleratorOverride else [inferenceAcceleratorOverride] } } } } } },
            '#withMemory':: d.fn(help='"The memory override for the task."', args=[d.arg(name='memory', type=d.T.string)]),
            withMemory(memory): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { memory: memory } } } } } },
            '#withTaskRoleArn':: d.fn(help='"The Amazon Resource Name (ARN) of the IAM role that containers in this task can assume. All containers in this task are granted the permissions that are specified in this role."', args=[d.arg(name='taskRoleArn', type=d.T.string)]),
            withTaskRoleArn(taskRoleArn): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { taskRoleArn: taskRoleArn } } } } } },
          },
          '#placementConstraint':: d.obj(help='"An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below."'),
          placementConstraint: {
            '#withExpression':: d.fn(help='"A cluster query language expression to apply to the constraint. You cannot specify an expression if the constraint type is distinctInstance. Maximum length of 2,000."', args=[d.arg(name='expression', type=d.T.string)]),
            withExpression(expression): { expression: expression },
            '#withType':: d.fn(help='"The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict the selection to a group of valid candidates. Valid Values: distinctInstance, memberOf."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#placementStrategy':: d.obj(help='"The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below."'),
          placementStrategy: {
            '#withField':: d.fn(help='"The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. For the binpack placement strategy, valid values are cpu and memory. For the random placement strategy, this field is not used. Maximum length of 255."', args=[d.arg(name='field', type=d.T.string)]),
            withField(field): { field: field },
            '#withType':: d.fn(help='"The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task). Valid Values: random, spread, binpack."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withCapacityProviderStrategy':: d.fn(help='"List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below."', args=[d.arg(name='capacityProviderStrategy', type=d.T.array)]),
          withCapacityProviderStrategy(capacityProviderStrategy): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { capacityProviderStrategy: if std.isArray(v=capacityProviderStrategy) then capacityProviderStrategy else [capacityProviderStrategy] } } } } },
          '#withCapacityProviderStrategyMixin':: d.fn(help='"List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='capacityProviderStrategy', type=d.T.array)]),
          withCapacityProviderStrategyMixin(capacityProviderStrategy): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { capacityProviderStrategy+: if std.isArray(v=capacityProviderStrategy) then capacityProviderStrategy else [capacityProviderStrategy] } } } } },
          '#withEnableEcsManagedTags':: d.fn(help='"Specifies whether to enable Amazon ECS managed tags for the task. Valid values: true, false."', args=[d.arg(name='enableEcsManagedTags', type=d.T.boolean)]),
          withEnableEcsManagedTags(enableEcsManagedTags): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { enableEcsManagedTags: enableEcsManagedTags } } } } },
          '#withEnableExecuteCommand':: d.fn(help='"Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Valid values: true, false."', args=[d.arg(name='enableExecuteCommand', type=d.T.boolean)]),
          withEnableExecuteCommand(enableExecuteCommand): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { enableExecuteCommand: enableExecuteCommand } } } } },
          '#withGroup':: d.fn(help='"Specifies an Amazon ECS task group for the task. The maximum length is 255 characters."', args=[d.arg(name='group', type=d.T.string)]),
          withGroup(group): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { group: group } } } } },
          '#withLaunchType':: d.fn(help='"Specifies the launch type on which your task is running. The launch type that you specify here must match one of the launch type (compatibilities) of the target task. The FARGATE value is supported only in the Regions where AWS Fargate with Amazon ECS is supported. Valid Values: EC2, FARGATE, EXTERNAL"', args=[d.arg(name='launchType', type=d.T.string)]),
          withLaunchType(launchType): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { launchType: launchType } } } } },
          '#withPlacementConstraint':: d.fn(help='"An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below."', args=[d.arg(name='placementConstraint', type=d.T.array)]),
          withPlacementConstraint(placementConstraint): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { placementConstraint: if std.isArray(v=placementConstraint) then placementConstraint else [placementConstraint] } } } } },
          '#withPlacementConstraintMixin':: d.fn(help='"An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='placementConstraint', type=d.T.array)]),
          withPlacementConstraintMixin(placementConstraint): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { placementConstraint+: if std.isArray(v=placementConstraint) then placementConstraint else [placementConstraint] } } } } },
          '#withPlacementStrategy':: d.fn(help='"The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below."', args=[d.arg(name='placementStrategy', type=d.T.array)]),
          withPlacementStrategy(placementStrategy): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { placementStrategy: if std.isArray(v=placementStrategy) then placementStrategy else [placementStrategy] } } } } },
          '#withPlacementStrategyMixin':: d.fn(help='"The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='placementStrategy', type=d.T.array)]),
          withPlacementStrategyMixin(placementStrategy): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { placementStrategy+: if std.isArray(v=placementStrategy) then placementStrategy else [placementStrategy] } } } } },
          '#withPlatformVersion':: d.fn(help='"Specifies the platform version for the task. Specify only the numeric portion of the platform version, such as 1.1.0. This structure is used only if LaunchType is FARGATE."', args=[d.arg(name='platformVersion', type=d.T.string)]),
          withPlatformVersion(platformVersion): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { platformVersion: platformVersion } } } } },
          '#withPropagateTags':: d.fn(help='"Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the TagResource API action. Valid Values: TASK_DEFINITION"', args=[d.arg(name='propagateTags', type=d.T.string)]),
          withPropagateTags(propagateTags): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { propagateTags: propagateTags } } } } },
          '#withReferenceId':: d.fn(help='"The reference ID to use for the task. Maximum length of 1,024."', args=[d.arg(name='referenceId', type=d.T.string)]),
          withReferenceId(referenceId): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { referenceId: referenceId } } } } },
          '#withTags':: d.fn(help='"Key-value map of tags that you apply to the task to help you categorize and organize them."', args=[d.arg(name='tags', type=d.T.object)]),
          withTags(tags): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { tags: tags } } } } },
          '#withTagsMixin':: d.fn(help='"Key-value map of tags that you apply to the task to help you categorize and organize them."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
          withTagsMixin(tags): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { tags+: tags } } } } },
          '#withTaskCount':: d.fn(help='"The number of tasks to create based on TaskDefinition. The default is 1."', args=[d.arg(name='taskCount', type=d.T.number)]),
          withTaskCount(taskCount): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { taskCount: taskCount } } } } },
          '#withTaskDefinitionArn':: d.fn(help='"The ARN of the task definition to use if the event target is an Amazon ECS task."', args=[d.arg(name='taskDefinitionArn', type=d.T.string)]),
          withTaskDefinitionArn(taskDefinitionArn): { spec+: { forProvider+: { targetParameters+: { ecsTaskParameters+: { taskDefinitionArn: taskDefinitionArn } } } } },
        },
        '#eventbridgeEventBusParameters':: d.obj(help='"The parameters for using an EventBridge event bus as a target. Detailed below."'),
        eventbridgeEventBusParameters: {
          '#withDetailType':: d.fn(help='"A free-form string, with a maximum of 128 characters, used to decide what fields to expect in the event detail."', args=[d.arg(name='detailType', type=d.T.string)]),
          withDetailType(detailType): { spec+: { forProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { detailType: detailType } } } } },
          '#withEndpointId':: d.fn(help='"The URL subdomain of the endpoint. For example, if the URL for Endpoint is https://abcde.veo.endpoints.event.amazonaws.com, then the EndpointId is abcde.veo."', args=[d.arg(name='endpointId', type=d.T.string)]),
          withEndpointId(endpointId): { spec+: { forProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { endpointId: endpointId } } } } },
          '#withResources':: d.fn(help='"List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present."', args=[d.arg(name='resources', type=d.T.array)]),
          withResources(resources): { spec+: { forProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { resources: if std.isArray(v=resources) then resources else [resources] } } } } },
          '#withResourcesMixin':: d.fn(help='"List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='resources', type=d.T.array)]),
          withResourcesMixin(resources): { spec+: { forProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { resources+: if std.isArray(v=resources) then resources else [resources] } } } } },
          '#withSource':: d.fn(help='"The source of the event. Maximum length of 256."', args=[d.arg(name='source', type=d.T.string)]),
          withSource(source): { spec+: { forProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { source: source } } } } },
          '#withTime':: d.fn(help='"The time stamp of the event, per RFC3339. If no time stamp is provided, the time stamp of the PutEvents call is used. This is the JSON path to the field in the event e.g. $.detail.timestamp"', args=[d.arg(name='time', type=d.T.string)]),
          withTime(time): { spec+: { forProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { time: time } } } } },
        },
        '#httpParameters':: d.obj(help='"These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below."'),
        httpParameters: {
          '#withHeaderParameters':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParameters(headerParameters): { spec+: { forProvider+: { targetParameters+: { httpParameters+: { headerParameters: headerParameters } } } } },
          '#withHeaderParametersMixin':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParametersMixin(headerParameters): { spec+: { forProvider+: { targetParameters+: { httpParameters+: { headerParameters+: headerParameters } } } } },
          '#withPathParameterValues':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValues(pathParameterValues): { spec+: { forProvider+: { targetParameters+: { httpParameters+: { pathParameterValues: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withPathParameterValuesMixin':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValuesMixin(pathParameterValues): { spec+: { forProvider+: { targetParameters+: { httpParameters+: { pathParameterValues+: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withQueryStringParameters':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParameters(queryStringParameters): { spec+: { forProvider+: { targetParameters+: { httpParameters+: { queryStringParameters: queryStringParameters } } } } },
          '#withQueryStringParametersMixin':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParametersMixin(queryStringParameters): { spec+: { forProvider+: { targetParameters+: { httpParameters+: { queryStringParameters+: queryStringParameters } } } } },
        },
        '#kinesisStreamParameters':: d.obj(help='"The parameters for using a Kinesis stream as a source. Detailed below."'),
        kinesisStreamParameters: {
          '#withPartitionKey':: d.fn(help='"Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream."', args=[d.arg(name='partitionKey', type=d.T.string)]),
          withPartitionKey(partitionKey): { spec+: { forProvider+: { targetParameters+: { kinesisStreamParameters+: { partitionKey: partitionKey } } } } },
        },
        '#lambdaFunctionParameters':: d.obj(help='"The parameters for using a Lambda function as a target. Detailed below."'),
        lambdaFunctionParameters: {
          '#withInvocationType':: d.fn(help='"Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET."', args=[d.arg(name='invocationType', type=d.T.string)]),
          withInvocationType(invocationType): { spec+: { forProvider+: { targetParameters+: { lambdaFunctionParameters+: { invocationType: invocationType } } } } },
        },
        '#redshiftDataParameters':: d.obj(help='"These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below."'),
        redshiftDataParameters: {
          '#withDatabase':: d.fn(help='"The name of the database. Required when authenticating using temporary credentials."', args=[d.arg(name='database', type=d.T.string)]),
          withDatabase(database): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { database: database } } } } },
          '#withDbUser':: d.fn(help='"The database user name. Required when authenticating using temporary credentials."', args=[d.arg(name='dbUser', type=d.T.string)]),
          withDbUser(dbUser): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { dbUser: dbUser } } } } },
          '#withSecretManagerArn':: d.fn(help='"The name or ARN of the secret that enables access to the database. Required when authenticating using Secrets Manager."', args=[d.arg(name='secretManagerArn', type=d.T.string)]),
          withSecretManagerArn(secretManagerArn): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { secretManagerArn: secretManagerArn } } } } },
          '#withSqls':: d.fn(help='"List of SQL statements text to run, each of maximum length of 100,000."', args=[d.arg(name='sqls', type=d.T.array)]),
          withSqls(sqls): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { sqls: if std.isArray(v=sqls) then sqls else [sqls] } } } } },
          '#withSqlsMixin':: d.fn(help='"List of SQL statements text to run, each of maximum length of 100,000."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sqls', type=d.T.array)]),
          withSqlsMixin(sqls): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { sqls+: if std.isArray(v=sqls) then sqls else [sqls] } } } } },
          '#withStatementName':: d.fn(help='"The name of the SQL statement. You can name the SQL statement when you create it to identify the query."', args=[d.arg(name='statementName', type=d.T.string)]),
          withStatementName(statementName): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { statementName: statementName } } } } },
          '#withWithEvent':: d.fn(help='"Indicates whether to send an event back to EventBridge after the SQL statement runs."', args=[d.arg(name='withEvent', type=d.T.boolean)]),
          withWithEvent(withEvent): { spec+: { forProvider+: { targetParameters+: { redshiftDataParameters+: { withEvent: withEvent } } } } },
        },
        '#sagemakerPipelineParameters':: d.obj(help='"The parameters for using a SageMaker AI pipeline as a target. Detailed below."'),
        sagemakerPipelineParameters: {
          '#pipelineParameter':: d.obj(help='"List of Parameter names and values for SageMaker AI Model Building Pipeline execution. Detailed below."'),
          pipelineParameter: {
            '#withName':: d.fn(help='"The name of the container that receives the override. This parameter is required if any override is specified."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
            '#withValue':: d.fn(help='"Value of parameter to start execution of a SageMaker AI Model Building Pipeline. Maximum length of 1024."', args=[d.arg(name='value', type=d.T.string)]),
            withValue(value): { value: value },
          },
          '#withPipelineParameter':: d.fn(help='"List of Parameter names and values for SageMaker AI Model Building Pipeline execution. Detailed below."', args=[d.arg(name='pipelineParameter', type=d.T.array)]),
          withPipelineParameter(pipelineParameter): { spec+: { forProvider+: { targetParameters+: { sagemakerPipelineParameters+: { pipelineParameter: if std.isArray(v=pipelineParameter) then pipelineParameter else [pipelineParameter] } } } } },
          '#withPipelineParameterMixin':: d.fn(help='"List of Parameter names and values for SageMaker AI Model Building Pipeline execution. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pipelineParameter', type=d.T.array)]),
          withPipelineParameterMixin(pipelineParameter): { spec+: { forProvider+: { targetParameters+: { sagemakerPipelineParameters+: { pipelineParameter+: if std.isArray(v=pipelineParameter) then pipelineParameter else [pipelineParameter] } } } } },
        },
        '#sqsQueueParameters':: d.obj(help='"The parameters for using a Amazon SQS stream as a target. Detailed below."'),
        sqsQueueParameters: {
          '#withMessageDeduplicationId':: d.fn(help='"This parameter applies only to FIFO (first-in-first-out) queues. The token used for deduplication of sent messages."', args=[d.arg(name='messageDeduplicationId', type=d.T.string)]),
          withMessageDeduplicationId(messageDeduplicationId): { spec+: { forProvider+: { targetParameters+: { sqsQueueParameters+: { messageDeduplicationId: messageDeduplicationId } } } } },
          '#withMessageGroupId':: d.fn(help='"The FIFO message group ID to use as the target."', args=[d.arg(name='messageGroupId', type=d.T.string)]),
          withMessageGroupId(messageGroupId): { spec+: { forProvider+: { targetParameters+: { sqsQueueParameters+: { messageGroupId: messageGroupId } } } } },
        },
        '#stepFunctionStateMachineParameters':: d.obj(help='"The parameters for using a Step Functions state machine as a target. Detailed below."'),
        stepFunctionStateMachineParameters: {
          '#withInvocationType':: d.fn(help='"Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET."', args=[d.arg(name='invocationType', type=d.T.string)]),
          withInvocationType(invocationType): { spec+: { forProvider+: { targetParameters+: { stepFunctionStateMachineParameters+: { invocationType: invocationType } } } } },
        },
        '#withInputTemplate':: d.fn(help='"Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters."', args=[d.arg(name='inputTemplate', type=d.T.string)]),
        withInputTemplate(inputTemplate): { spec+: { forProvider+: { targetParameters+: { inputTemplate: inputTemplate } } } },
      },
      '#targetRef':: d.obj(help='"Reference to a Queue in sqs to populate target."'),
      targetRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { targetRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { targetRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { forProvider+: { targetRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { targetRef+: { namespace: namespace } } } },
      },
      '#targetSelector':: d.obj(help='"Selector for a Queue in sqs to populate target."'),
      targetSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { forProvider+: { targetSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { forProvider+: { targetSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { forProvider+: { targetSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { forProvider+: { targetSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { forProvider+: { targetSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { forProvider+: { targetSelector+: { namespace: namespace } } } },
      },
      '#withDescription':: d.fn(help='"A description of the pipe. At most 512 characters."', args=[d.arg(name='description', type=d.T.string)]),
      withDescription(description): { spec+: { forProvider+: { description: description } } },
      '#withDesiredState':: d.fn(help='"The state the pipe should be in. One of: RUNNING, STOPPED."', args=[d.arg(name='desiredState', type=d.T.string)]),
      withDesiredState(desiredState): { spec+: { forProvider+: { desiredState: desiredState } } },
      '#withEnrichment':: d.fn(help='"Enrichment resource of the pipe (typically an ARN). Read more about enrichment in the User Guide."', args=[d.arg(name='enrichment', type=d.T.string)]),
      withEnrichment(enrichment): { spec+: { forProvider+: { enrichment: enrichment } } },
      '#withKmsKeyIdentifier':: d.fn(help='"Identifier of the AWS KMS customer managed key for EventBridge to use, if you choose to use a customer managed key to encrypt pipe data. The identifier can be the key Amazon Resource Name (ARN), KeyId, key alias, or key alias ARN. If not set, EventBridge uses an AWS owned key to encrypt pipe data."', args=[d.arg(name='kmsKeyIdentifier', type=d.T.string)]),
      withKmsKeyIdentifier(kmsKeyIdentifier): { spec+: { forProvider+: { kmsKeyIdentifier: kmsKeyIdentifier } } },
      '#withRegion':: d.fn(help="\"Region where this resource will be managed. Defaults to the Region set in the provider configuration.\\nRegion is the region you'd like your resource to be created in.\"", args=[d.arg(name='region', type=d.T.string)]),
      withRegion(region): { spec+: { forProvider+: { region: region } } },
      '#withRoleArn':: d.fn(help='"ARN of the role that allows the pipe to send data to the target."', args=[d.arg(name='roleArn', type=d.T.string)]),
      withRoleArn(roleArn): { spec+: { forProvider+: { roleArn: roleArn } } },
      '#withSource':: d.fn(help="\"Source resource of the pipe. This field typically requires an ARN (Amazon Resource Name). However, when using a self-managed Kafka cluster, you should use a different format. Instead of an ARN, use 'smk://' followed by the bootstrap server's address.\"", args=[d.arg(name='source', type=d.T.string)]),
      withSource(source): { spec+: { forProvider+: { source: source } } },
      '#withTags':: d.fn(help='"Key-value map of resource tags."', args=[d.arg(name='tags', type=d.T.object)]),
      withTags(tags): { spec+: { forProvider+: { tags: tags } } },
      '#withTagsMixin':: d.fn(help='"Key-value map of resource tags."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
      withTagsMixin(tags): { spec+: { forProvider+: { tags+: tags } } },
      '#withTarget':: d.fn(help='"Target resource of the pipe (typically an ARN)."', args=[d.arg(name='target', type=d.T.string)]),
      withTarget(target): { spec+: { forProvider+: { target: target } } },
    },
    '#initProvider':: d.obj(help='"THIS IS A BETA FIELD. It will be honored\\nunless the Management Policies feature flag is disabled.\\nInitProvider holds the same fields as ForProvider, with the exception\\nof Identifier and other resource reference fields. The fields that are\\nin InitProvider are merged into ForProvider when the resource is created.\\nThe same fields are also added to the terraform ignore_changes hook, to\\navoid updating them after creation. This is useful for fields that are\\nrequired on creation, but we do not desire to update them after creation,\\nfor example because of an external controller is managing them, like an\\nautoscaler."'),
    initProvider: {
      '#enrichmentParameters':: d.obj(help='"Parameters to configure enrichment for your pipe. Detailed below."'),
      enrichmentParameters: {
        '#httpParameters':: d.obj(help="\"Contains the HTTP parameters to use when the target is a API Gateway REST endpoint or EventBridge ApiDestination. If you specify an API Gateway REST API or EventBridge ApiDestination as a target, you can use this parameter to specify headers, path parameters, and query string keys/values as part of your target invoking request. If you're using ApiDestinations, the corresponding Connection can also have these values configured. In case of any conflicting keys, values from the Connection take precedence. Detailed below.\""),
        httpParameters: {
          '#withHeaderParameters':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParameters(headerParameters): { spec+: { initProvider+: { enrichmentParameters+: { httpParameters+: { headerParameters: headerParameters } } } } },
          '#withHeaderParametersMixin':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParametersMixin(headerParameters): { spec+: { initProvider+: { enrichmentParameters+: { httpParameters+: { headerParameters+: headerParameters } } } } },
          '#withPathParameterValues':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValues(pathParameterValues): { spec+: { initProvider+: { enrichmentParameters+: { httpParameters+: { pathParameterValues: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withPathParameterValuesMixin':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValuesMixin(pathParameterValues): { spec+: { initProvider+: { enrichmentParameters+: { httpParameters+: { pathParameterValues+: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withQueryStringParameters':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParameters(queryStringParameters): { spec+: { initProvider+: { enrichmentParameters+: { httpParameters+: { queryStringParameters: queryStringParameters } } } } },
          '#withQueryStringParametersMixin':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParametersMixin(queryStringParameters): { spec+: { initProvider+: { enrichmentParameters+: { httpParameters+: { queryStringParameters+: queryStringParameters } } } } },
        },
        '#withInputTemplate':: d.fn(help='"Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters."', args=[d.arg(name='inputTemplate', type=d.T.string)]),
        withInputTemplate(inputTemplate): { spec+: { initProvider+: { enrichmentParameters+: { inputTemplate: inputTemplate } } } },
      },
      '#enrichmentRef':: d.obj(help='"Reference to a APIDestination in cloudwatchevents to populate enrichment."'),
      enrichmentRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { enrichmentRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { enrichmentRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { initProvider+: { enrichmentRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { enrichmentRef+: { namespace: namespace } } } },
      },
      '#enrichmentSelector':: d.obj(help='"Selector for a APIDestination in cloudwatchevents to populate enrichment."'),
      enrichmentSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { enrichmentSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { enrichmentSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { initProvider+: { enrichmentSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { initProvider+: { enrichmentSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { initProvider+: { enrichmentSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { enrichmentSelector+: { namespace: namespace } } } },
      },
      '#logConfiguration':: d.obj(help='"Logging configuration settings for the pipe. Detailed below."'),
      logConfiguration: {
        '#cloudwatchLogsLogDestination':: d.obj(help='"Amazon CloudWatch Logs logging configuration settings for the pipe. Detailed below."'),
        cloudwatchLogsLogDestination: {
          '#logGroupArnRef':: d.obj(help='"Reference to a Group in cloudwatchlogs to populate logGroupArn."'),
          logGroupArnRef: {
            '#policy':: d.obj(help='"Policies for referencing."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { policy+: { resolution: resolution } } } } } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { policy+: { resolve: resolve } } } } } } },
            },
            '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { name: name } } } } } },
            '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
            withNamespace(namespace): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnRef+: { namespace: namespace } } } } } },
          },
          '#logGroupArnSelector':: d.obj(help='"Selector for a Group in cloudwatchlogs to populate logGroupArn."'),
          logGroupArnSelector: {
            '#policy':: d.obj(help='"Policies for selection."'),
            policy: {
              '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
              withResolution(resolution): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { policy+: { resolution: resolution } } } } } } },
              '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
              withResolve(resolve): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { policy+: { resolve: resolve } } } } } } },
            },
            '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
            withMatchControllerRef(matchControllerRef): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { matchControllerRef: matchControllerRef } } } } } },
            '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabels(matchLabels): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { matchLabels: matchLabels } } } } } },
            '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
            withMatchLabelsMixin(matchLabels): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { matchLabels+: matchLabels } } } } } },
            '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
            withNamespace(namespace): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArnSelector+: { namespace: namespace } } } } } },
          },
          '#withLogGroupArn':: d.fn(help='"Amazon Web Services Resource Name (ARN) for the CloudWatch log group to which EventBridge sends the log records."', args=[d.arg(name='logGroupArn', type=d.T.string)]),
          withLogGroupArn(logGroupArn): { spec+: { initProvider+: { logConfiguration+: { cloudwatchLogsLogDestination+: { logGroupArn: logGroupArn } } } } },
        },
        '#firehoseLogDestination':: d.obj(help='"Amazon Kinesis Data Firehose logging configuration settings for the pipe. Detailed below."'),
        firehoseLogDestination: {
          '#withDeliveryStreamArn':: d.fn(help='"Amazon Resource Name (ARN) of the Kinesis Data Firehose delivery stream to which EventBridge delivers the pipe log records."', args=[d.arg(name='deliveryStreamArn', type=d.T.string)]),
          withDeliveryStreamArn(deliveryStreamArn): { spec+: { initProvider+: { logConfiguration+: { firehoseLogDestination+: { deliveryStreamArn: deliveryStreamArn } } } } },
        },
        '#s3LogDestination':: d.obj(help='"Amazon S3 logging configuration settings for the pipe. Detailed below."'),
        s3LogDestination: {
          '#withBucketName':: d.fn(help='"Name of the Amazon S3 bucket to which EventBridge delivers the log records for the pipe."', args=[d.arg(name='bucketName', type=d.T.string)]),
          withBucketName(bucketName): { spec+: { initProvider+: { logConfiguration+: { s3LogDestination+: { bucketName: bucketName } } } } },
          '#withBucketOwner':: d.fn(help='"Amazon Web Services account that owns the Amazon S3 bucket to which EventBridge delivers the log records for the pipe."', args=[d.arg(name='bucketOwner', type=d.T.string)]),
          withBucketOwner(bucketOwner): { spec+: { initProvider+: { logConfiguration+: { s3LogDestination+: { bucketOwner: bucketOwner } } } } },
          '#withOutputFormat':: d.fn(help='"EventBridge format for the log records. Valid values json, plain and w3c."', args=[d.arg(name='outputFormat', type=d.T.string)]),
          withOutputFormat(outputFormat): { spec+: { initProvider+: { logConfiguration+: { s3LogDestination+: { outputFormat: outputFormat } } } } },
          '#withPrefix':: d.fn(help='"Prefix text with which to begin Amazon S3 log object names."', args=[d.arg(name='prefix', type=d.T.string)]),
          withPrefix(prefix): { spec+: { initProvider+: { logConfiguration+: { s3LogDestination+: { prefix: prefix } } } } },
        },
        '#withIncludeExecutionData':: d.fn(help='"String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL."', args=[d.arg(name='includeExecutionData', type=d.T.array)]),
        withIncludeExecutionData(includeExecutionData): { spec+: { initProvider+: { logConfiguration+: { includeExecutionData: if std.isArray(v=includeExecutionData) then includeExecutionData else [includeExecutionData] } } } },
        '#withIncludeExecutionDataMixin':: d.fn(help='"String list that specifies whether the execution data (specifically, the payload, awsRequest, and awsResponse fields) is included in the log messages for this pipe. This applies to all log destinations for the pipe. Valid values ALL."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='includeExecutionData', type=d.T.array)]),
        withIncludeExecutionDataMixin(includeExecutionData): { spec+: { initProvider+: { logConfiguration+: { includeExecutionData+: if std.isArray(v=includeExecutionData) then includeExecutionData else [includeExecutionData] } } } },
        '#withLevel':: d.fn(help='"The level of logging detail to include. Valid values OFF, ERROR, INFO and TRACE."', args=[d.arg(name='level', type=d.T.string)]),
        withLevel(level): { spec+: { initProvider+: { logConfiguration+: { level: level } } } },
      },
      '#roleArnRef':: d.obj(help='"Reference to a Role in iam to populate roleArn."'),
      roleArnRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { roleArnRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { roleArnRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { initProvider+: { roleArnRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { roleArnRef+: { namespace: namespace } } } },
      },
      '#roleArnSelector':: d.obj(help='"Selector for a Role in iam to populate roleArn."'),
      roleArnSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { roleArnSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { roleArnSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { initProvider+: { roleArnSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { initProvider+: { roleArnSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { initProvider+: { roleArnSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { roleArnSelector+: { namespace: namespace } } } },
      },
      '#sourceParameters':: d.obj(help='"Parameters to configure a source for the pipe. Detailed below."'),
      sourceParameters: {
        '#activemqBrokerParameters':: d.obj(help='"The parameters for using an Active MQ broker as a source. Detailed below."'),
        activemqBrokerParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withBasicAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the basic auth credentials."', args=[d.arg(name='basicAuth', type=d.T.string)]),
            withBasicAuth(basicAuth): { spec+: { initProvider+: { sourceParameters+: { activemqBrokerParameters+: { credentials+: { basicAuth: basicAuth } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { activemqBrokerParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { activemqBrokerParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withQueueName':: d.fn(help='"The name of the destination queue to consume. Maximum length of 1000."', args=[d.arg(name='queueName', type=d.T.string)]),
          withQueueName(queueName): { spec+: { initProvider+: { sourceParameters+: { activemqBrokerParameters+: { queueName: queueName } } } } },
        },
        '#dynamodbStreamParameters':: d.obj(help='"The parameters for using a DynamoDB stream as a source.  Detailed below."'),
        dynamodbStreamParameters: {
          '#deadLetterConfig':: d.obj(help='"Define the target queue to send dead-letter queue events to. Detailed below."'),
          deadLetterConfig: {
            '#withArn':: d.fn(help='"The ARN of the Amazon SQS queue specified as the target for the dead-letter queue."', args=[d.arg(name='arn', type=d.T.string)]),
            withArn(arn): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { deadLetterConfig+: { arn: arn } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withMaximumRecordAgeInSeconds':: d.fn(help='"Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800."', args=[d.arg(name='maximumRecordAgeInSeconds', type=d.T.number)]),
          withMaximumRecordAgeInSeconds(maximumRecordAgeInSeconds): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { maximumRecordAgeInSeconds: maximumRecordAgeInSeconds } } } } },
          '#withMaximumRetryAttempts':: d.fn(help='"Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000."', args=[d.arg(name='maximumRetryAttempts', type=d.T.number)]),
          withMaximumRetryAttempts(maximumRetryAttempts): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { maximumRetryAttempts: maximumRetryAttempts } } } } },
          '#withOnPartialBatchItemFailure':: d.fn(help='"Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT."', args=[d.arg(name='onPartialBatchItemFailure', type=d.T.string)]),
          withOnPartialBatchItemFailure(onPartialBatchItemFailure): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { onPartialBatchItemFailure: onPartialBatchItemFailure } } } } },
          '#withParallelizationFactor':: d.fn(help='"The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10."', args=[d.arg(name='parallelizationFactor', type=d.T.number)]),
          withParallelizationFactor(parallelizationFactor): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { parallelizationFactor: parallelizationFactor } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { initProvider+: { sourceParameters+: { dynamodbStreamParameters+: { startingPosition: startingPosition } } } } },
        },
        '#filterCriteria':: d.obj(help='"The collection of event patterns used to filter events. Detailed below."'),
        filterCriteria: {
          '#filter':: d.obj(help='"An array of up to 5 event patterns. Detailed below."'),
          filter: {
            '#withPattern':: d.fn(help='"The event pattern. At most 4096 characters."', args=[d.arg(name='pattern', type=d.T.string)]),
            withPattern(pattern): { pattern: pattern },
          },
          '#withFilter':: d.fn(help='"An array of up to 5 event patterns. Detailed below."', args=[d.arg(name='filter', type=d.T.array)]),
          withFilter(filter): { spec+: { initProvider+: { sourceParameters+: { filterCriteria+: { filter: if std.isArray(v=filter) then filter else [filter] } } } } },
          '#withFilterMixin':: d.fn(help='"An array of up to 5 event patterns. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='filter', type=d.T.array)]),
          withFilterMixin(filter): { spec+: { initProvider+: { sourceParameters+: { filterCriteria+: { filter+: if std.isArray(v=filter) then filter else [filter] } } } } },
        },
        '#kinesisStreamParameters':: d.obj(help='"The parameters for using a Kinesis stream as a source. Detailed below."'),
        kinesisStreamParameters: {
          '#deadLetterConfig':: d.obj(help='"Define the target queue to send dead-letter queue events to. Detailed below."'),
          deadLetterConfig: {
            '#withArn':: d.fn(help='"The ARN of the Amazon SQS queue specified as the target for the dead-letter queue."', args=[d.arg(name='arn', type=d.T.string)]),
            withArn(arn): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { deadLetterConfig+: { arn: arn } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withMaximumRecordAgeInSeconds':: d.fn(help='"Discard records older than the specified age. The default value is -1, which sets the maximum age to infinite. When the value is set to infinite, EventBridge never discards old records. Maximum value of 604,800."', args=[d.arg(name='maximumRecordAgeInSeconds', type=d.T.number)]),
          withMaximumRecordAgeInSeconds(maximumRecordAgeInSeconds): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { maximumRecordAgeInSeconds: maximumRecordAgeInSeconds } } } } },
          '#withMaximumRetryAttempts':: d.fn(help='"Discard records after the specified number of retries. The default value is -1, which sets the maximum number of retries to infinite. When MaximumRetryAttempts is infinite, EventBridge retries failed records until the record expires in the event source. Maximum value of 10,000."', args=[d.arg(name='maximumRetryAttempts', type=d.T.number)]),
          withMaximumRetryAttempts(maximumRetryAttempts): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { maximumRetryAttempts: maximumRetryAttempts } } } } },
          '#withOnPartialBatchItemFailure':: d.fn(help='"Define how to handle item process failures. AUTOMATIC_BISECT halves each batch and retry each half until all the records are processed or there is one failed message left in the batch. Valid values: AUTOMATIC_BISECT."', args=[d.arg(name='onPartialBatchItemFailure', type=d.T.string)]),
          withOnPartialBatchItemFailure(onPartialBatchItemFailure): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { onPartialBatchItemFailure: onPartialBatchItemFailure } } } } },
          '#withParallelizationFactor':: d.fn(help='"The number of batches to process concurrently from each shard. The default value is 1. Maximum value of 10."', args=[d.arg(name='parallelizationFactor', type=d.T.number)]),
          withParallelizationFactor(parallelizationFactor): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { parallelizationFactor: parallelizationFactor } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST, AT_TIMESTAMP."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { startingPosition: startingPosition } } } } },
          '#withStartingPositionTimestamp':: d.fn(help='"With StartingPosition set to AT_TIMESTAMP, the time from which to start reading, in Unix time seconds."', args=[d.arg(name='startingPositionTimestamp', type=d.T.string)]),
          withStartingPositionTimestamp(startingPositionTimestamp): { spec+: { initProvider+: { sourceParameters+: { kinesisStreamParameters+: { startingPositionTimestamp: startingPositionTimestamp } } } } },
        },
        '#managedStreamingKafkaParameters':: d.obj(help='"The parameters for using an MSK stream as a source. Detailed below."'),
        managedStreamingKafkaParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withClientCertificateTlsAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='clientCertificateTlsAuth', type=d.T.string)]),
            withClientCertificateTlsAuth(clientCertificateTlsAuth): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { credentials+: { clientCertificateTlsAuth: clientCertificateTlsAuth } } } } } },
            '#withSaslScram512Auth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='saslScram512Auth', type=d.T.string)]),
            withSaslScram512Auth(saslScram512Auth): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { credentials+: { saslScram512Auth: saslScram512Auth } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { batchSize: batchSize } } } } },
          '#withConsumerGroupId':: d.fn(help='"The name of the destination queue to consume. Maximum value of 200."', args=[d.arg(name='consumerGroupId', type=d.T.string)]),
          withConsumerGroupId(consumerGroupId): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { consumerGroupId: consumerGroupId } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { startingPosition: startingPosition } } } } },
          '#withTopicName':: d.fn(help='"The name of the topic that the pipe will read from. Maximum length of 249."', args=[d.arg(name='topicName', type=d.T.string)]),
          withTopicName(topicName): { spec+: { initProvider+: { sourceParameters+: { managedStreamingKafkaParameters+: { topicName: topicName } } } } },
        },
        '#rabbitmqBrokerParameters':: d.obj(help='"The parameters for using a Rabbit MQ broker as a source. Detailed below."'),
        rabbitmqBrokerParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withBasicAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='basicAuth', type=d.T.string)]),
            withBasicAuth(basicAuth): { spec+: { initProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { credentials+: { basicAuth: basicAuth } } } } } },
          },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withQueueName':: d.fn(help='"The name of the destination queue to consume. Maximum length of 1000."', args=[d.arg(name='queueName', type=d.T.string)]),
          withQueueName(queueName): { spec+: { initProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { queueName: queueName } } } } },
          '#withVirtualHost':: d.fn(help='"The name of the virtual host associated with the source broker. Maximum length of 200."', args=[d.arg(name='virtualHost', type=d.T.string)]),
          withVirtualHost(virtualHost): { spec+: { initProvider+: { sourceParameters+: { rabbitmqBrokerParameters+: { virtualHost: virtualHost } } } } },
        },
        '#selfManagedKafkaParameters':: d.obj(help='"The parameters for using a self-managed Apache Kafka stream as a source. Detailed below."'),
        selfManagedKafkaParameters: {
          '#credentials':: d.obj(help='"The credentials needed to access the resource. Detailed below."'),
          credentials: {
            '#withBasicAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='basicAuth', type=d.T.string)]),
            withBasicAuth(basicAuth): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { basicAuth: basicAuth } } } } } },
            '#withClientCertificateTlsAuth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='clientCertificateTlsAuth', type=d.T.string)]),
            withClientCertificateTlsAuth(clientCertificateTlsAuth): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { clientCertificateTlsAuth: clientCertificateTlsAuth } } } } } },
            '#withSaslScram256Auth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='saslScram256Auth', type=d.T.string)]),
            withSaslScram256Auth(saslScram256Auth): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { saslScram256Auth: saslScram256Auth } } } } } },
            '#withSaslScram512Auth':: d.fn(help='"The ARN of the Secrets Manager secret containing the credentials."', args=[d.arg(name='saslScram512Auth', type=d.T.string)]),
            withSaslScram512Auth(saslScram512Auth): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { credentials+: { saslScram512Auth: saslScram512Auth } } } } } },
          },
          '#vpc':: d.obj(help='"This structure specifies the VPC subnets and security groups for the stream, and whether a public IP address is to be used. Detailed below."'),
          vpc: {
            '#withSecurityGroups':: d.fn(help='"List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."', args=[d.arg(name='securityGroups', type=d.T.array)]),
            withSecurityGroups(securityGroups): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { securityGroups: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } },
            '#withSecurityGroupsMixin':: d.fn(help='"List of security groups associated with the stream. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='securityGroups', type=d.T.array)]),
            withSecurityGroupsMixin(securityGroups): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { securityGroups+: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } },
            '#withSubnets':: d.fn(help='"List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets."', args=[d.arg(name='subnets', type=d.T.array)]),
            withSubnets(subnets): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { subnets: if std.isArray(v=subnets) then subnets else [subnets] } } } } } },
            '#withSubnetsMixin':: d.fn(help='"List of the subnets associated with the stream. These subnets must all be in the same VPC. You can specify as many as 16 subnets."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='subnets', type=d.T.array)]),
            withSubnetsMixin(subnets): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { vpc+: { subnets+: if std.isArray(v=subnets) then subnets else [subnets] } } } } } },
          },
          '#withAdditionalBootstrapServers':: d.fn(help='"An array of server URLs. Maximum number of 2 items, each of maximum length 300."', args=[d.arg(name='additionalBootstrapServers', type=d.T.array)]),
          withAdditionalBootstrapServers(additionalBootstrapServers): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { additionalBootstrapServers: if std.isArray(v=additionalBootstrapServers) then additionalBootstrapServers else [additionalBootstrapServers] } } } } },
          '#withAdditionalBootstrapServersMixin':: d.fn(help='"An array of server URLs. Maximum number of 2 items, each of maximum length 300."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='additionalBootstrapServers', type=d.T.array)]),
          withAdditionalBootstrapServersMixin(additionalBootstrapServers): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { additionalBootstrapServers+: if std.isArray(v=additionalBootstrapServers) then additionalBootstrapServers else [additionalBootstrapServers] } } } } },
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { batchSize: batchSize } } } } },
          '#withConsumerGroupId':: d.fn(help='"The name of the destination queue to consume. Maximum value of 200."', args=[d.arg(name='consumerGroupId', type=d.T.string)]),
          withConsumerGroupId(consumerGroupId): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { consumerGroupId: consumerGroupId } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
          '#withServerRootCaCertificate':: d.fn(help='"The ARN of the Secrets Manager secret used for certification."', args=[d.arg(name='serverRootCaCertificate', type=d.T.string)]),
          withServerRootCaCertificate(serverRootCaCertificate): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { serverRootCaCertificate: serverRootCaCertificate } } } } },
          '#withStartingPosition':: d.fn(help='"The position in a stream from which to start reading. Valid values: TRIM_HORIZON, LATEST."', args=[d.arg(name='startingPosition', type=d.T.string)]),
          withStartingPosition(startingPosition): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { startingPosition: startingPosition } } } } },
          '#withTopicName':: d.fn(help='"The name of the topic that the pipe will read from. Maximum length of 249."', args=[d.arg(name='topicName', type=d.T.string)]),
          withTopicName(topicName): { spec+: { initProvider+: { sourceParameters+: { selfManagedKafkaParameters+: { topicName: topicName } } } } },
        },
        '#sqsQueueParameters':: d.obj(help='"The parameters for using a Amazon SQS stream as a source. Detailed below."'),
        sqsQueueParameters: {
          '#withBatchSize':: d.fn(help='"The maximum number of records to include in each batch. Maximum value of 10000."', args=[d.arg(name='batchSize', type=d.T.number)]),
          withBatchSize(batchSize): { spec+: { initProvider+: { sourceParameters+: { sqsQueueParameters+: { batchSize: batchSize } } } } },
          '#withMaximumBatchingWindowInSeconds':: d.fn(help='"The maximum length of a time to wait for events. Maximum value of 300."', args=[d.arg(name='maximumBatchingWindowInSeconds', type=d.T.number)]),
          withMaximumBatchingWindowInSeconds(maximumBatchingWindowInSeconds): { spec+: { initProvider+: { sourceParameters+: { sqsQueueParameters+: { maximumBatchingWindowInSeconds: maximumBatchingWindowInSeconds } } } } },
        },
      },
      '#sourceRef':: d.obj(help='"Reference to a Queue in sqs to populate source."'),
      sourceRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { sourceRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { sourceRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { initProvider+: { sourceRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { sourceRef+: { namespace: namespace } } } },
      },
      '#sourceSelector':: d.obj(help='"Selector for a Queue in sqs to populate source."'),
      sourceSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { sourceSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { sourceSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { initProvider+: { sourceSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { initProvider+: { sourceSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { initProvider+: { sourceSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { sourceSelector+: { namespace: namespace } } } },
      },
      '#targetParameters':: d.obj(help='"Parameters to configure a target for your pipe. Detailed below."'),
      targetParameters: {
        '#batchJobParameters':: d.obj(help='"The parameters for using an AWS Batch job as a target. Detailed below."'),
        batchJobParameters: {
          '#arrayProperties':: d.obj(help='"The array properties for the submitted job, such as the size of the array. The array size can be between 2 and 10,000. If you specify array properties for a job, it becomes an array job. This parameter is used only if the target is an AWS Batch job. Detailed below."'),
          arrayProperties: {
            '#withSize':: d.fn(help='"The size of the array, if this is an array batch job. Minimum value of 2. Maximum value of 10,000."', args=[d.arg(name='size', type=d.T.number)]),
            withSize(size): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { arrayProperties+: { size: size } } } } } },
          },
          '#containerOverrides':: d.obj(help='"The overrides that are sent to a container. Detailed below."'),
          containerOverrides: {
            '#environment':: d.obj(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with \\" AWS Batch \\". This naming convention is reserved for variables that AWS Batch sets. Detailed below."'),
            environment: {
              '#withName':: d.fn(help='"The name of the key-value pair. For environment variables, this is the name of the environment variable."', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { name: name },
              '#withValue':: d.fn(help='"The value of the key-value pair. For environment variables, this is the value of the environment variable."', args=[d.arg(name='value', type=d.T.string)]),
              withValue(value): { value: value },
            },
            '#resourceRequirement':: d.obj(help='"The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below."'),
            resourceRequirement: {
              '#withType':: d.fn(help='"The type of resource to assign to a container. The supported resources include GPU, MEMORY, and VCPU."', args=[d.arg(name='type', type=d.T.string)]),
              withType(type): { type: type },
              '#withValue':: d.fn(help='"The quantity of the specified resource to reserve for the container. The values vary based on the type specified."', args=[d.arg(name='value', type=d.T.string)]),
              withValue(value): { value: value },
            },
            '#withCommand':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition."', args=[d.arg(name='command', type=d.T.array)]),
            withCommand(command): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { command: if std.isArray(v=command) then command else [command] } } } } } },
            '#withCommandMixin':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='command', type=d.T.array)]),
            withCommandMixin(command): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { command+: if std.isArray(v=command) then command else [command] } } } } } },
            '#withEnvironment':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with \\" AWS Batch \\". This naming convention is reserved for variables that AWS Batch sets. Detailed below."', args=[d.arg(name='environment', type=d.T.array)]),
            withEnvironment(environment): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { environment: if std.isArray(v=environment) then environment else [environment] } } } } } },
            '#withEnvironmentMixin':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. Environment variables cannot start with \\" AWS Batch \\". This naming convention is reserved for variables that AWS Batch sets. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='environment', type=d.T.array)]),
            withEnvironmentMixin(environment): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { environment+: if std.isArray(v=environment) then environment else [environment] } } } } } },
            '#withInstanceType':: d.fn(help="\"The instance type to use for a multi-node parallel job. This parameter isn't applicable to single-node container jobs or jobs that run on Fargate resources, and shouldn't be provided.\"", args=[d.arg(name='instanceType', type=d.T.string)]),
            withInstanceType(instanceType): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { instanceType: instanceType } } } } } },
            '#withResourceRequirement':: d.fn(help='"The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below."', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
            withResourceRequirement(resourceRequirement): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { resourceRequirement: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] } } } } } },
            '#withResourceRequirementMixin':: d.fn(help='"The type and amount of resources to assign to a container. This overrides the settings in the job definition. The supported resources include GPU, MEMORY, and VCPU. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
            withResourceRequirementMixin(resourceRequirement): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { containerOverrides+: { resourceRequirement+: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] } } } } } },
          },
          '#dependsOn':: d.obj(help='"A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below."'),
          dependsOn: {
            '#withJobId':: d.fn(help="\"The job ID of the AWS Batch job that's associated with this dependency.\"", args=[d.arg(name='jobId', type=d.T.string)]),
            withJobId(jobId): { jobId: jobId },
            '#withType':: d.fn(help='"The type of the job dependency. Valid Values: N_TO_N, SEQUENTIAL."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#retryStrategy':: d.obj(help='"The retry strategy to use for failed jobs. When a retry strategy is specified here, it overrides the retry strategy defined in the job definition. Detailed below."'),
          retryStrategy: {
            '#withAttempts':: d.fn(help='"The number of times to move a job to the RUNNABLE status. If the value of attempts is greater than one, the job is retried on failure the same number of attempts as the value. Maximum value of 10."', args=[d.arg(name='attempts', type=d.T.number)]),
            withAttempts(attempts): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { retryStrategy+: { attempts: attempts } } } } } },
          },
          '#withDependsOn':: d.fn(help='"A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below."', args=[d.arg(name='dependsOn', type=d.T.array)]),
          withDependsOn(dependsOn): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { dependsOn: if std.isArray(v=dependsOn) then dependsOn else [dependsOn] } } } } },
          '#withDependsOnMixin':: d.fn(help='"A list of dependencies for the job. A job can depend upon a maximum of 20 jobs. You can specify a SEQUENTIAL type dependency without specifying a job ID for array jobs so that each child array job completes sequentially, starting at index 0. You can also specify an N_TO_N type dependency with a job ID for array jobs. In that case, each index child of this job must wait for the corresponding index child of each dependency to complete before it can begin. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='dependsOn', type=d.T.array)]),
          withDependsOnMixin(dependsOn): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { dependsOn+: if std.isArray(v=dependsOn) then dependsOn else [dependsOn] } } } } },
          '#withJobDefinition':: d.fn(help='"The job definition used by this job. This value can be one of name, name:revision, or the Amazon Resource Name (ARN) for the job definition. If name is specified without a revision then the latest active revision is used."', args=[d.arg(name='jobDefinition', type=d.T.string)]),
          withJobDefinition(jobDefinition): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { jobDefinition: jobDefinition } } } } },
          '#withJobName':: d.fn(help='"The name of the job. It can be up to 128 letters long."', args=[d.arg(name='jobName', type=d.T.string)]),
          withJobName(jobName): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { jobName: jobName } } } } },
          '#withParameters':: d.fn(help='"Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below."', args=[d.arg(name='parameters', type=d.T.object)]),
          withParameters(parameters): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { parameters: parameters } } } } },
          '#withParametersMixin':: d.fn(help='"Additional parameters passed to the job that replace parameter substitution placeholders that are set in the job definition. Parameters are specified as a key and value pair mapping. Parameters included here override any corresponding parameter defaults from the job definition. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='parameters', type=d.T.object)]),
          withParametersMixin(parameters): { spec+: { initProvider+: { targetParameters+: { batchJobParameters+: { parameters+: parameters } } } } },
        },
        '#cloudwatchLogsParameters':: d.obj(help='"The parameters for using an CloudWatch Logs log stream as a target. Detailed below."'),
        cloudwatchLogsParameters: {
          '#withLogStreamName':: d.fn(help='"The name of the log stream."', args=[d.arg(name='logStreamName', type=d.T.string)]),
          withLogStreamName(logStreamName): { spec+: { initProvider+: { targetParameters+: { cloudwatchLogsParameters+: { logStreamName: logStreamName } } } } },
          '#withTimestamp':: d.fn(help='"The time the event occurred, expressed as the number of milliseconds after Jan 1, 1970 00:00:00 UTC. This is the JSON path to the field in the event e.g. $.detail.timestamp"', args=[d.arg(name='timestamp', type=d.T.string)]),
          withTimestamp(timestamp): { spec+: { initProvider+: { targetParameters+: { cloudwatchLogsParameters+: { timestamp: timestamp } } } } },
        },
        '#ecsTaskParameters':: d.obj(help='"The parameters for using an Amazon ECS task as a target. Detailed below."'),
        ecsTaskParameters: {
          '#capacityProviderStrategy':: d.obj(help='"List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below."'),
          capacityProviderStrategy: {
            '#withBase':: d.fn(help='"The base value designates how many tasks, at a minimum, to run on the specified capacity provider. Only one capacity provider in a capacity provider strategy can have a base defined. If no value is specified, the default value of 0 is used. Maximum value of 100,000."', args=[d.arg(name='base', type=d.T.number)]),
            withBase(base): { base: base },
            '#withCapacityProvider':: d.fn(help='"The short name of the capacity provider. Maximum value of 255."', args=[d.arg(name='capacityProvider', type=d.T.string)]),
            withCapacityProvider(capacityProvider): { capacityProvider: capacityProvider },
            '#withWeight':: d.fn(help='"The weight value designates the relative percentage of the total number of tasks launched that should use the specified capacity provider. The weight value is taken into consideration after the base value, if defined, is satisfied. Maximum value of 1,000."', args=[d.arg(name='weight', type=d.T.number)]),
            withWeight(weight): { weight: weight },
          },
          '#networkConfiguration':: d.obj(help='"Use this structure if the Amazon ECS task uses the awsvpc network mode. This structure specifies the VPC subnets and security groups associated with the task, and whether a public IP address is to be used. This structure is required if LaunchType is FARGATE because the awsvpc mode is required for Fargate tasks. If you specify NetworkConfiguration when the target ECS task does not use the awsvpc network mode, the task fails. Detailed below."'),
          networkConfiguration: {
            '#awsVpcConfiguration':: d.obj(help='"Use this structure to specify the VPC subnets and security groups for the task, and whether a public IP address is to be used. This structure is relevant only for ECS tasks that use the awsvpc network mode. Detailed below."'),
            awsVpcConfiguration: {
              '#withAssignPublicIp':: d.fn(help="\"Specifies whether the task's elastic network interface receives a public IP address. You can specify ENABLED only when LaunchType in EcsParameters is set to FARGATE. Valid Values: ENABLED, DISABLED.\"", args=[d.arg(name='assignPublicIp', type=d.T.string)]),
              withAssignPublicIp(assignPublicIp): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { assignPublicIp: assignPublicIp } } } } } } },
              '#withSecurityGroups':: d.fn(help='"Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."', args=[d.arg(name='securityGroups', type=d.T.array)]),
              withSecurityGroups(securityGroups): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { securityGroups: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } } },
              '#withSecurityGroupsMixin':: d.fn(help='"Specifies the security groups associated with the task. These security groups must all be in the same VPC. You can specify as many as five security groups. If you do not specify a security group, the default security group for the VPC is used."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='securityGroups', type=d.T.array)]),
              withSecurityGroupsMixin(securityGroups): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { securityGroups+: if std.isArray(v=securityGroups) then securityGroups else [securityGroups] } } } } } } },
              '#withSubnets':: d.fn(help='"Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets."', args=[d.arg(name='subnets', type=d.T.array)]),
              withSubnets(subnets): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { subnets: if std.isArray(v=subnets) then subnets else [subnets] } } } } } } },
              '#withSubnetsMixin':: d.fn(help='"Specifies the subnets associated with the task. These subnets must all be in the same VPC. You can specify as many as 16 subnets."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='subnets', type=d.T.array)]),
              withSubnetsMixin(subnets): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { networkConfiguration+: { awsVpcConfiguration+: { subnets+: if std.isArray(v=subnets) then subnets else [subnets] } } } } } } },
            },
          },
          '#overrides':: d.obj(help='"The overrides that are associated with a task. Detailed below."'),
          overrides: {
            '#containerOverride':: d.obj(help='"One or more container overrides that are sent to a task. Detailed below."'),
            containerOverride: {
              '#environment':: d.obj(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below."'),
              environment: {
                '#withName':: d.fn(help='"The name of the key-value pair. For environment variables, this is the name of the environment variable."', args=[d.arg(name='name', type=d.T.string)]),
                withName(name): { name: name },
                '#withValue':: d.fn(help='"The value of the key-value pair. For environment variables, this is the value of the environment variable."', args=[d.arg(name='value', type=d.T.string)]),
                withValue(value): { value: value },
              },
              '#environmentFile':: d.obj(help='"A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below."'),
              environmentFile: {
                '#withType':: d.fn(help='"The file type to use. The only supported value is s3."', args=[d.arg(name='type', type=d.T.string)]),
                withType(type): { type: type },
                '#withValue':: d.fn(help='"The Amazon Resource Name (ARN) of the Amazon S3 object containing the environment variable file."', args=[d.arg(name='value', type=d.T.string)]),
                withValue(value): { value: value },
              },
              '#resourceRequirement':: d.obj(help='"The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below."'),
              resourceRequirement: {
                '#withType':: d.fn(help='"The type of resource to assign to a container. The supported values are GPU or InferenceAccelerator."', args=[d.arg(name='type', type=d.T.string)]),
                withType(type): { type: type },
                '#withValue':: d.fn(help="\"The value for the specified resource type. If the GPU type is used, the value is the number of physical GPUs the Amazon ECS container agent reserves for the container. The number of GPUs that's reserved for all containers in a task can't exceed the number of available GPUs on the container instance that the task is launched on. If the InferenceAccelerator type is used, the value matches the deviceName for an InferenceAccelerator specified in a task definition.\"", args=[d.arg(name='value', type=d.T.string)]),
                withValue(value): { value: value },
              },
              '#withCommand':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name."', args=[d.arg(name='command', type=d.T.array)]),
              withCommand(command): { command: if std.isArray(v=command) then command else [command] },
              '#withCommandMixin':: d.fn(help='"List of commands to send to the container that overrides the default command from the Docker image or the task definition. You must also specify a container name."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='command', type=d.T.array)]),
              withCommandMixin(command): { command+: if std.isArray(v=command) then command else [command] },
              '#withCpu':: d.fn(help='"The number of cpu units reserved for the container, instead of the default value from the task definition. You must also specify a container name."', args=[d.arg(name='cpu', type=d.T.number)]),
              withCpu(cpu): { cpu: cpu },
              '#withEnvironment':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below."', args=[d.arg(name='environment', type=d.T.array)]),
              withEnvironment(environment): { environment: if std.isArray(v=environment) then environment else [environment] },
              '#withEnvironmentFile':: d.fn(help='"A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below."', args=[d.arg(name='environmentFile', type=d.T.array)]),
              withEnvironmentFile(environmentFile): { environmentFile: if std.isArray(v=environmentFile) then environmentFile else [environmentFile] },
              '#withEnvironmentFileMixin':: d.fn(help='"A list of files containing the environment variables to pass to a container, instead of the value from the container definition. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='environmentFile', type=d.T.array)]),
              withEnvironmentFileMixin(environmentFile): { environmentFile+: if std.isArray(v=environmentFile) then environmentFile else [environmentFile] },
              '#withEnvironmentMixin':: d.fn(help='"The environment variables to send to the container. You can add new environment variables, which are added to the container at launch, or you can override the existing environment variables from the Docker image or the task definition. You must also specify a container name. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='environment', type=d.T.array)]),
              withEnvironmentMixin(environment): { environment+: if std.isArray(v=environment) then environment else [environment] },
              '#withMemory':: d.fn(help='"The hard limit (in MiB) of memory to present to the container, instead of the default value from the task definition. If your container attempts to exceed the memory specified here, the container is killed. You must also specify a container name."', args=[d.arg(name='memory', type=d.T.number)]),
              withMemory(memory): { memory: memory },
              '#withMemoryReservation':: d.fn(help='"The soft limit (in MiB) of memory to reserve for the container, instead of the default value from the task definition. You must also specify a container name."', args=[d.arg(name='memoryReservation', type=d.T.number)]),
              withMemoryReservation(memoryReservation): { memoryReservation: memoryReservation },
              '#withName':: d.fn(help='"The name of the container that receives the override. This parameter is required if any override is specified."', args=[d.arg(name='name', type=d.T.string)]),
              withName(name): { name: name },
              '#withResourceRequirement':: d.fn(help='"The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below."', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
              withResourceRequirement(resourceRequirement): { resourceRequirement: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] },
              '#withResourceRequirementMixin':: d.fn(help='"The type and amount of a resource to assign to a container, instead of the default value from the task definition. The only supported resource is a GPU. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='resourceRequirement', type=d.T.array)]),
              withResourceRequirementMixin(resourceRequirement): { resourceRequirement+: if std.isArray(v=resourceRequirement) then resourceRequirement else [resourceRequirement] },
            },
            '#ephemeralStorage':: d.obj(help='"The ephemeral storage setting override for the task.  Detailed below."'),
            ephemeralStorage: {
              '#withSizeInGib':: d.fn(help='"The total amount, in GiB, of ephemeral storage to set for the task. The minimum supported value is 21 GiB and the maximum supported value is 200 GiB."', args=[d.arg(name='sizeInGib', type=d.T.number)]),
              withSizeInGib(sizeInGib): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { ephemeralStorage+: { sizeInGib: sizeInGib } } } } } } },
            },
            '#inferenceAcceleratorOverride':: d.obj(help='"List of Elastic Inference accelerator overrides for the task. Detailed below."'),
            inferenceAcceleratorOverride: {
              '#withDeviceName':: d.fn(help='"The Elastic Inference accelerator device name to override for the task. This parameter must match a deviceName specified in the task definition."', args=[d.arg(name='deviceName', type=d.T.string)]),
              withDeviceName(deviceName): { deviceName: deviceName },
              '#withDeviceType':: d.fn(help='"The Elastic Inference accelerator type to use."', args=[d.arg(name='deviceType', type=d.T.string)]),
              withDeviceType(deviceType): { deviceType: deviceType },
            },
            '#withContainerOverride':: d.fn(help='"One or more container overrides that are sent to a task. Detailed below."', args=[d.arg(name='containerOverride', type=d.T.array)]),
            withContainerOverride(containerOverride): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { containerOverride: if std.isArray(v=containerOverride) then containerOverride else [containerOverride] } } } } } },
            '#withContainerOverrideMixin':: d.fn(help='"One or more container overrides that are sent to a task. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='containerOverride', type=d.T.array)]),
            withContainerOverrideMixin(containerOverride): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { containerOverride+: if std.isArray(v=containerOverride) then containerOverride else [containerOverride] } } } } } },
            '#withCpu':: d.fn(help='"The cpu override for the task."', args=[d.arg(name='cpu', type=d.T.string)]),
            withCpu(cpu): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { cpu: cpu } } } } } },
            '#withExecutionRoleArn':: d.fn(help='"The Amazon Resource Name (ARN) of the task execution IAM role override for the task."', args=[d.arg(name='executionRoleArn', type=d.T.string)]),
            withExecutionRoleArn(executionRoleArn): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { executionRoleArn: executionRoleArn } } } } } },
            '#withInferenceAcceleratorOverride':: d.fn(help='"List of Elastic Inference accelerator overrides for the task. Detailed below."', args=[d.arg(name='inferenceAcceleratorOverride', type=d.T.array)]),
            withInferenceAcceleratorOverride(inferenceAcceleratorOverride): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { inferenceAcceleratorOverride: if std.isArray(v=inferenceAcceleratorOverride) then inferenceAcceleratorOverride else [inferenceAcceleratorOverride] } } } } } },
            '#withInferenceAcceleratorOverrideMixin':: d.fn(help='"List of Elastic Inference accelerator overrides for the task. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='inferenceAcceleratorOverride', type=d.T.array)]),
            withInferenceAcceleratorOverrideMixin(inferenceAcceleratorOverride): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { inferenceAcceleratorOverride+: if std.isArray(v=inferenceAcceleratorOverride) then inferenceAcceleratorOverride else [inferenceAcceleratorOverride] } } } } } },
            '#withMemory':: d.fn(help='"The memory override for the task."', args=[d.arg(name='memory', type=d.T.string)]),
            withMemory(memory): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { memory: memory } } } } } },
            '#withTaskRoleArn':: d.fn(help='"The Amazon Resource Name (ARN) of the IAM role that containers in this task can assume. All containers in this task are granted the permissions that are specified in this role."', args=[d.arg(name='taskRoleArn', type=d.T.string)]),
            withTaskRoleArn(taskRoleArn): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { overrides+: { taskRoleArn: taskRoleArn } } } } } },
          },
          '#placementConstraint':: d.obj(help='"An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below."'),
          placementConstraint: {
            '#withExpression':: d.fn(help='"A cluster query language expression to apply to the constraint. You cannot specify an expression if the constraint type is distinctInstance. Maximum length of 2,000."', args=[d.arg(name='expression', type=d.T.string)]),
            withExpression(expression): { expression: expression },
            '#withType':: d.fn(help='"The type of constraint. Use distinctInstance to ensure that each task in a particular group is running on a different container instance. Use memberOf to restrict the selection to a group of valid candidates. Valid Values: distinctInstance, memberOf."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#placementStrategy':: d.obj(help='"The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below."'),
          placementStrategy: {
            '#withField':: d.fn(help='"The field to apply the placement strategy against. For the spread placement strategy, valid values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs.availability-zone. For the binpack placement strategy, valid values are cpu and memory. For the random placement strategy, this field is not used. Maximum length of 255."', args=[d.arg(name='field', type=d.T.string)]),
            withField(field): { field: field },
            '#withType':: d.fn(help='"The type of placement strategy. The random placement strategy randomly places tasks on available candidates. The spread placement strategy spreads placement across available candidates evenly based on the field parameter. The binpack strategy places tasks on available candidates that have the least available amount of the resource that is specified with the field parameter. For example, if you binpack on memory, a task is placed on the instance with the least amount of remaining memory (but still enough to run the task). Valid Values: random, spread, binpack."', args=[d.arg(name='type', type=d.T.string)]),
            withType(type): { type: type },
          },
          '#withCapacityProviderStrategy':: d.fn(help='"List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below."', args=[d.arg(name='capacityProviderStrategy', type=d.T.array)]),
          withCapacityProviderStrategy(capacityProviderStrategy): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { capacityProviderStrategy: if std.isArray(v=capacityProviderStrategy) then capacityProviderStrategy else [capacityProviderStrategy] } } } } },
          '#withCapacityProviderStrategyMixin':: d.fn(help='"List of capacity provider strategies to use for the task. If a capacityProviderStrategy is specified, the launchType parameter must be omitted. If no capacityProviderStrategy or launchType is specified, the defaultCapacityProviderStrategy for the cluster is used. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='capacityProviderStrategy', type=d.T.array)]),
          withCapacityProviderStrategyMixin(capacityProviderStrategy): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { capacityProviderStrategy+: if std.isArray(v=capacityProviderStrategy) then capacityProviderStrategy else [capacityProviderStrategy] } } } } },
          '#withEnableEcsManagedTags':: d.fn(help='"Specifies whether to enable Amazon ECS managed tags for the task. Valid values: true, false."', args=[d.arg(name='enableEcsManagedTags', type=d.T.boolean)]),
          withEnableEcsManagedTags(enableEcsManagedTags): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { enableEcsManagedTags: enableEcsManagedTags } } } } },
          '#withEnableExecuteCommand':: d.fn(help='"Whether or not to enable the execute command functionality for the containers in this task. If true, this enables execute command functionality on all containers in the task. Valid values: true, false."', args=[d.arg(name='enableExecuteCommand', type=d.T.boolean)]),
          withEnableExecuteCommand(enableExecuteCommand): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { enableExecuteCommand: enableExecuteCommand } } } } },
          '#withGroup':: d.fn(help='"Specifies an Amazon ECS task group for the task. The maximum length is 255 characters."', args=[d.arg(name='group', type=d.T.string)]),
          withGroup(group): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { group: group } } } } },
          '#withLaunchType':: d.fn(help='"Specifies the launch type on which your task is running. The launch type that you specify here must match one of the launch type (compatibilities) of the target task. The FARGATE value is supported only in the Regions where AWS Fargate with Amazon ECS is supported. Valid Values: EC2, FARGATE, EXTERNAL"', args=[d.arg(name='launchType', type=d.T.string)]),
          withLaunchType(launchType): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { launchType: launchType } } } } },
          '#withPlacementConstraint':: d.fn(help='"An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below."', args=[d.arg(name='placementConstraint', type=d.T.array)]),
          withPlacementConstraint(placementConstraint): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { placementConstraint: if std.isArray(v=placementConstraint) then placementConstraint else [placementConstraint] } } } } },
          '#withPlacementConstraintMixin':: d.fn(help='"An array of placement constraint objects to use for the task. You can specify up to 10 constraints per task (including constraints in the task definition and those specified at runtime). Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='placementConstraint', type=d.T.array)]),
          withPlacementConstraintMixin(placementConstraint): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { placementConstraint+: if std.isArray(v=placementConstraint) then placementConstraint else [placementConstraint] } } } } },
          '#withPlacementStrategy':: d.fn(help='"The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below."', args=[d.arg(name='placementStrategy', type=d.T.array)]),
          withPlacementStrategy(placementStrategy): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { placementStrategy: if std.isArray(v=placementStrategy) then placementStrategy else [placementStrategy] } } } } },
          '#withPlacementStrategyMixin':: d.fn(help='"The placement strategy objects to use for the task. You can specify a maximum of five strategy rules per task. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='placementStrategy', type=d.T.array)]),
          withPlacementStrategyMixin(placementStrategy): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { placementStrategy+: if std.isArray(v=placementStrategy) then placementStrategy else [placementStrategy] } } } } },
          '#withPlatformVersion':: d.fn(help='"Specifies the platform version for the task. Specify only the numeric portion of the platform version, such as 1.1.0. This structure is used only if LaunchType is FARGATE."', args=[d.arg(name='platformVersion', type=d.T.string)]),
          withPlatformVersion(platformVersion): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { platformVersion: platformVersion } } } } },
          '#withPropagateTags':: d.fn(help='"Specifies whether to propagate the tags from the task definition to the task. If no value is specified, the tags are not propagated. Tags can only be propagated to the task during task creation. To add tags to a task after task creation, use the TagResource API action. Valid Values: TASK_DEFINITION"', args=[d.arg(name='propagateTags', type=d.T.string)]),
          withPropagateTags(propagateTags): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { propagateTags: propagateTags } } } } },
          '#withReferenceId':: d.fn(help='"The reference ID to use for the task. Maximum length of 1,024."', args=[d.arg(name='referenceId', type=d.T.string)]),
          withReferenceId(referenceId): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { referenceId: referenceId } } } } },
          '#withTags':: d.fn(help='"Key-value map of tags that you apply to the task to help you categorize and organize them."', args=[d.arg(name='tags', type=d.T.object)]),
          withTags(tags): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { tags: tags } } } } },
          '#withTagsMixin':: d.fn(help='"Key-value map of tags that you apply to the task to help you categorize and organize them."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
          withTagsMixin(tags): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { tags+: tags } } } } },
          '#withTaskCount':: d.fn(help='"The number of tasks to create based on TaskDefinition. The default is 1."', args=[d.arg(name='taskCount', type=d.T.number)]),
          withTaskCount(taskCount): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { taskCount: taskCount } } } } },
          '#withTaskDefinitionArn':: d.fn(help='"The ARN of the task definition to use if the event target is an Amazon ECS task."', args=[d.arg(name='taskDefinitionArn', type=d.T.string)]),
          withTaskDefinitionArn(taskDefinitionArn): { spec+: { initProvider+: { targetParameters+: { ecsTaskParameters+: { taskDefinitionArn: taskDefinitionArn } } } } },
        },
        '#eventbridgeEventBusParameters':: d.obj(help='"The parameters for using an EventBridge event bus as a target. Detailed below."'),
        eventbridgeEventBusParameters: {
          '#withDetailType':: d.fn(help='"A free-form string, with a maximum of 128 characters, used to decide what fields to expect in the event detail."', args=[d.arg(name='detailType', type=d.T.string)]),
          withDetailType(detailType): { spec+: { initProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { detailType: detailType } } } } },
          '#withEndpointId':: d.fn(help='"The URL subdomain of the endpoint. For example, if the URL for Endpoint is https://abcde.veo.endpoints.event.amazonaws.com, then the EndpointId is abcde.veo."', args=[d.arg(name='endpointId', type=d.T.string)]),
          withEndpointId(endpointId): { spec+: { initProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { endpointId: endpointId } } } } },
          '#withResources':: d.fn(help='"List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present."', args=[d.arg(name='resources', type=d.T.array)]),
          withResources(resources): { spec+: { initProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { resources: if std.isArray(v=resources) then resources else [resources] } } } } },
          '#withResourcesMixin':: d.fn(help='"List of AWS resources, identified by Amazon Resource Name (ARN), which the event primarily concerns. Any number, including zero, may be present."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='resources', type=d.T.array)]),
          withResourcesMixin(resources): { spec+: { initProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { resources+: if std.isArray(v=resources) then resources else [resources] } } } } },
          '#withSource':: d.fn(help='"The source of the event. Maximum length of 256."', args=[d.arg(name='source', type=d.T.string)]),
          withSource(source): { spec+: { initProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { source: source } } } } },
          '#withTime':: d.fn(help='"The time stamp of the event, per RFC3339. If no time stamp is provided, the time stamp of the PutEvents call is used. This is the JSON path to the field in the event e.g. $.detail.timestamp"', args=[d.arg(name='time', type=d.T.string)]),
          withTime(time): { spec+: { initProvider+: { targetParameters+: { eventbridgeEventBusParameters+: { time: time } } } } },
        },
        '#httpParameters':: d.obj(help='"These are custom parameter to be used when the target is an API Gateway REST APIs or EventBridge ApiDestinations. Detailed below."'),
        httpParameters: {
          '#withHeaderParameters':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParameters(headerParameters): { spec+: { initProvider+: { targetParameters+: { httpParameters+: { headerParameters: headerParameters } } } } },
          '#withHeaderParametersMixin':: d.fn(help='"Key-value mapping of the headers that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headerParameters', type=d.T.object)]),
          withHeaderParametersMixin(headerParameters): { spec+: { initProvider+: { targetParameters+: { httpParameters+: { headerParameters+: headerParameters } } } } },
          '#withPathParameterValues':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValues(pathParameterValues): { spec+: { initProvider+: { targetParameters+: { httpParameters+: { pathParameterValues: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withPathParameterValuesMixin':: d.fn(help='"The path parameter values to be used to populate API Gateway REST API or EventBridge ApiDestination path wildcards (\\"*\\")."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pathParameterValues', type=d.T.array)]),
          withPathParameterValuesMixin(pathParameterValues): { spec+: { initProvider+: { targetParameters+: { httpParameters+: { pathParameterValues+: if std.isArray(v=pathParameterValues) then pathParameterValues else [pathParameterValues] } } } } },
          '#withQueryStringParameters':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParameters(queryStringParameters): { spec+: { initProvider+: { targetParameters+: { httpParameters+: { queryStringParameters: queryStringParameters } } } } },
          '#withQueryStringParametersMixin':: d.fn(help='"Key-value mapping of the query strings that need to be sent as part of request invoking the API Gateway REST API or EventBridge ApiDestination. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='queryStringParameters', type=d.T.object)]),
          withQueryStringParametersMixin(queryStringParameters): { spec+: { initProvider+: { targetParameters+: { httpParameters+: { queryStringParameters+: queryStringParameters } } } } },
        },
        '#kinesisStreamParameters':: d.obj(help='"The parameters for using a Kinesis stream as a source. Detailed below."'),
        kinesisStreamParameters: {
          '#withPartitionKey':: d.fn(help='"Determines which shard in the stream the data record is assigned to. Partition keys are Unicode strings with a maximum length limit of 256 characters for each key. Amazon Kinesis Data Streams uses the partition key as input to a hash function that maps the partition key and associated data to a specific shard. Specifically, an MD5 hash function is used to map partition keys to 128-bit integer values and to map associated data records to shards. As a result of this hashing mechanism, all data records with the same partition key map to the same shard within the stream."', args=[d.arg(name='partitionKey', type=d.T.string)]),
          withPartitionKey(partitionKey): { spec+: { initProvider+: { targetParameters+: { kinesisStreamParameters+: { partitionKey: partitionKey } } } } },
        },
        '#lambdaFunctionParameters':: d.obj(help='"The parameters for using a Lambda function as a target. Detailed below."'),
        lambdaFunctionParameters: {
          '#withInvocationType':: d.fn(help='"Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET."', args=[d.arg(name='invocationType', type=d.T.string)]),
          withInvocationType(invocationType): { spec+: { initProvider+: { targetParameters+: { lambdaFunctionParameters+: { invocationType: invocationType } } } } },
        },
        '#redshiftDataParameters':: d.obj(help='"These are custom parameters to be used when the target is a Amazon Redshift cluster to invoke the Amazon Redshift Data API BatchExecuteStatement. Detailed below."'),
        redshiftDataParameters: {
          '#withDatabase':: d.fn(help='"The name of the database. Required when authenticating using temporary credentials."', args=[d.arg(name='database', type=d.T.string)]),
          withDatabase(database): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { database: database } } } } },
          '#withDbUser':: d.fn(help='"The database user name. Required when authenticating using temporary credentials."', args=[d.arg(name='dbUser', type=d.T.string)]),
          withDbUser(dbUser): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { dbUser: dbUser } } } } },
          '#withSecretManagerArn':: d.fn(help='"The name or ARN of the secret that enables access to the database. Required when authenticating using Secrets Manager."', args=[d.arg(name='secretManagerArn', type=d.T.string)]),
          withSecretManagerArn(secretManagerArn): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { secretManagerArn: secretManagerArn } } } } },
          '#withSqls':: d.fn(help='"List of SQL statements text to run, each of maximum length of 100,000."', args=[d.arg(name='sqls', type=d.T.array)]),
          withSqls(sqls): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { sqls: if std.isArray(v=sqls) then sqls else [sqls] } } } } },
          '#withSqlsMixin':: d.fn(help='"List of SQL statements text to run, each of maximum length of 100,000."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='sqls', type=d.T.array)]),
          withSqlsMixin(sqls): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { sqls+: if std.isArray(v=sqls) then sqls else [sqls] } } } } },
          '#withStatementName':: d.fn(help='"The name of the SQL statement. You can name the SQL statement when you create it to identify the query."', args=[d.arg(name='statementName', type=d.T.string)]),
          withStatementName(statementName): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { statementName: statementName } } } } },
          '#withWithEvent':: d.fn(help='"Indicates whether to send an event back to EventBridge after the SQL statement runs."', args=[d.arg(name='withEvent', type=d.T.boolean)]),
          withWithEvent(withEvent): { spec+: { initProvider+: { targetParameters+: { redshiftDataParameters+: { withEvent: withEvent } } } } },
        },
        '#sagemakerPipelineParameters':: d.obj(help='"The parameters for using a SageMaker AI pipeline as a target. Detailed below."'),
        sagemakerPipelineParameters: {
          '#pipelineParameter':: d.obj(help='"List of Parameter names and values for SageMaker AI Model Building Pipeline execution. Detailed below."'),
          pipelineParameter: {
            '#withName':: d.fn(help='"The name of the container that receives the override. This parameter is required if any override is specified."', args=[d.arg(name='name', type=d.T.string)]),
            withName(name): { name: name },
            '#withValue':: d.fn(help='"Value of parameter to start execution of a SageMaker AI Model Building Pipeline. Maximum length of 1024."', args=[d.arg(name='value', type=d.T.string)]),
            withValue(value): { value: value },
          },
          '#withPipelineParameter':: d.fn(help='"List of Parameter names and values for SageMaker AI Model Building Pipeline execution. Detailed below."', args=[d.arg(name='pipelineParameter', type=d.T.array)]),
          withPipelineParameter(pipelineParameter): { spec+: { initProvider+: { targetParameters+: { sagemakerPipelineParameters+: { pipelineParameter: if std.isArray(v=pipelineParameter) then pipelineParameter else [pipelineParameter] } } } } },
          '#withPipelineParameterMixin':: d.fn(help='"List of Parameter names and values for SageMaker AI Model Building Pipeline execution. Detailed below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pipelineParameter', type=d.T.array)]),
          withPipelineParameterMixin(pipelineParameter): { spec+: { initProvider+: { targetParameters+: { sagemakerPipelineParameters+: { pipelineParameter+: if std.isArray(v=pipelineParameter) then pipelineParameter else [pipelineParameter] } } } } },
        },
        '#sqsQueueParameters':: d.obj(help='"The parameters for using a Amazon SQS stream as a target. Detailed below."'),
        sqsQueueParameters: {
          '#withMessageDeduplicationId':: d.fn(help='"This parameter applies only to FIFO (first-in-first-out) queues. The token used for deduplication of sent messages."', args=[d.arg(name='messageDeduplicationId', type=d.T.string)]),
          withMessageDeduplicationId(messageDeduplicationId): { spec+: { initProvider+: { targetParameters+: { sqsQueueParameters+: { messageDeduplicationId: messageDeduplicationId } } } } },
          '#withMessageGroupId':: d.fn(help='"The FIFO message group ID to use as the target."', args=[d.arg(name='messageGroupId', type=d.T.string)]),
          withMessageGroupId(messageGroupId): { spec+: { initProvider+: { targetParameters+: { sqsQueueParameters+: { messageGroupId: messageGroupId } } } } },
        },
        '#stepFunctionStateMachineParameters':: d.obj(help='"The parameters for using a Step Functions state machine as a target. Detailed below."'),
        stepFunctionStateMachineParameters: {
          '#withInvocationType':: d.fn(help='"Specify whether to invoke the function synchronously or asynchronously. Valid Values: REQUEST_RESPONSE, FIRE_AND_FORGET."', args=[d.arg(name='invocationType', type=d.T.string)]),
          withInvocationType(invocationType): { spec+: { initProvider+: { targetParameters+: { stepFunctionStateMachineParameters+: { invocationType: invocationType } } } } },
        },
        '#withInputTemplate':: d.fn(help='"Valid JSON text passed to the target. In this case, nothing from the event itself is passed to the target. Maximum length of 8192 characters."', args=[d.arg(name='inputTemplate', type=d.T.string)]),
        withInputTemplate(inputTemplate): { spec+: { initProvider+: { targetParameters+: { inputTemplate: inputTemplate } } } },
      },
      '#targetRef':: d.obj(help='"Reference to a Queue in sqs to populate target."'),
      targetRef: {
        '#policy':: d.obj(help='"Policies for referencing."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { targetRef+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { targetRef+: { policy+: { resolve: resolve } } } } },
        },
        '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { spec+: { initProvider+: { targetRef+: { name: name } } } },
        '#withNamespace':: d.fn(help='"Namespace of the referenced object"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { targetRef+: { namespace: namespace } } } },
      },
      '#targetSelector':: d.obj(help='"Selector for a Queue in sqs to populate target."'),
      targetSelector: {
        '#policy':: d.obj(help='"Policies for selection."'),
        policy: {
          '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
          withResolution(resolution): { spec+: { initProvider+: { targetSelector+: { policy+: { resolution: resolution } } } } },
          '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
          withResolve(resolve): { spec+: { initProvider+: { targetSelector+: { policy+: { resolve: resolve } } } } },
        },
        '#withMatchControllerRef':: d.fn(help='"MatchControllerRef ensures an object with the same controller reference\\nas the selecting object is selected."', args=[d.arg(name='matchControllerRef', type=d.T.boolean)]),
        withMatchControllerRef(matchControllerRef): { spec+: { initProvider+: { targetSelector+: { matchControllerRef: matchControllerRef } } } },
        '#withMatchLabels':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabels(matchLabels): { spec+: { initProvider+: { targetSelector+: { matchLabels: matchLabels } } } },
        '#withMatchLabelsMixin':: d.fn(help='"MatchLabels ensures an object with matching labels is selected."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='matchLabels', type=d.T.object)]),
        withMatchLabelsMixin(matchLabels): { spec+: { initProvider+: { targetSelector+: { matchLabels+: matchLabels } } } },
        '#withNamespace':: d.fn(help='"Namespace for the selector"', args=[d.arg(name='namespace', type=d.T.string)]),
        withNamespace(namespace): { spec+: { initProvider+: { targetSelector+: { namespace: namespace } } } },
      },
      '#withDescription':: d.fn(help='"A description of the pipe. At most 512 characters."', args=[d.arg(name='description', type=d.T.string)]),
      withDescription(description): { spec+: { initProvider+: { description: description } } },
      '#withDesiredState':: d.fn(help='"The state the pipe should be in. One of: RUNNING, STOPPED."', args=[d.arg(name='desiredState', type=d.T.string)]),
      withDesiredState(desiredState): { spec+: { initProvider+: { desiredState: desiredState } } },
      '#withEnrichment':: d.fn(help='"Enrichment resource of the pipe (typically an ARN). Read more about enrichment in the User Guide."', args=[d.arg(name='enrichment', type=d.T.string)]),
      withEnrichment(enrichment): { spec+: { initProvider+: { enrichment: enrichment } } },
      '#withKmsKeyIdentifier':: d.fn(help='"Identifier of the AWS KMS customer managed key for EventBridge to use, if you choose to use a customer managed key to encrypt pipe data. The identifier can be the key Amazon Resource Name (ARN), KeyId, key alias, or key alias ARN. If not set, EventBridge uses an AWS owned key to encrypt pipe data."', args=[d.arg(name='kmsKeyIdentifier', type=d.T.string)]),
      withKmsKeyIdentifier(kmsKeyIdentifier): { spec+: { initProvider+: { kmsKeyIdentifier: kmsKeyIdentifier } } },
      '#withRoleArn':: d.fn(help='"ARN of the role that allows the pipe to send data to the target."', args=[d.arg(name='roleArn', type=d.T.string)]),
      withRoleArn(roleArn): { spec+: { initProvider+: { roleArn: roleArn } } },
      '#withSource':: d.fn(help="\"Source resource of the pipe. This field typically requires an ARN (Amazon Resource Name). However, when using a self-managed Kafka cluster, you should use a different format. Instead of an ARN, use 'smk://' followed by the bootstrap server's address.\"", args=[d.arg(name='source', type=d.T.string)]),
      withSource(source): { spec+: { initProvider+: { source: source } } },
      '#withTags':: d.fn(help='"Key-value map of resource tags."', args=[d.arg(name='tags', type=d.T.object)]),
      withTags(tags): { spec+: { initProvider+: { tags: tags } } },
      '#withTagsMixin':: d.fn(help='"Key-value map of resource tags."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
      withTagsMixin(tags): { spec+: { initProvider+: { tags+: tags } } },
      '#withTarget':: d.fn(help='"Target resource of the pipe (typically an ARN)."', args=[d.arg(name='target', type=d.T.string)]),
      withTarget(target): { spec+: { initProvider+: { target: target } } },
    },
    '#providerConfigRef':: d.obj(help='"ProviderConfigReference specifies how the provider that will be used to\\ncreate, observe, update, and delete this managed resource should be\\nconfigured."'),
    providerConfigRef: {
      '#withKind':: d.fn(help='"Kind of the referenced object."', args=[d.arg(name='kind', type=d.T.string)]),
      withKind(kind): { spec+: { providerConfigRef+: { kind: kind } } },
      '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { providerConfigRef+: { name: name } } },
    },
    '#withManagementPolicies':: d.fn(help='"THIS IS A BETA FIELD. It is on by default but can be opted out\\nthrough a Crossplane feature flag.\\nManagementPolicies specify the array of actions Crossplane is allowed to\\ntake on the managed and external resources.\\nSee the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223\\nand this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md"', args=[d.arg(name='managementPolicies', type=d.T.array)]),
    withManagementPolicies(managementPolicies): { spec+: { managementPolicies: if std.isArray(v=managementPolicies) then managementPolicies else [managementPolicies] } },
    '#withManagementPoliciesMixin':: d.fn(help='"THIS IS A BETA FIELD. It is on by default but can be opted out\\nthrough a Crossplane feature flag.\\nManagementPolicies specify the array of actions Crossplane is allowed to\\ntake on the managed and external resources.\\nSee the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223\\nand this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='managementPolicies', type=d.T.array)]),
    withManagementPoliciesMixin(managementPolicies): { spec+: { managementPolicies+: if std.isArray(v=managementPolicies) then managementPolicies else [managementPolicies] } },
    '#writeConnectionSecretToRef':: d.obj(help='"WriteConnectionSecretToReference specifies the namespace and name of a\\nSecret to which any connection details for this managed resource should\\nbe written. Connection details frequently include the endpoint, username,\\nand password required to connect to the managed resource."'),
    writeConnectionSecretToRef: {
      '#withName':: d.fn(help='"Name of the secret."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { writeConnectionSecretToRef+: { name: name } } },
    },
  },
  '#mixin': 'ignore',
  mixin: self,
}
