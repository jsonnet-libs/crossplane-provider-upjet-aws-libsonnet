{
  local d = (import 'doc-util/main.libsonnet'),
  '#':: d.pkg(name='table', url='', help='"Table is the Schema for the Tables API. Provides a DynamoDB table resource"'),
  '#metadata':: d.obj(help='"ObjectMeta is metadata that all persisted resources must have, which includes all objects users must create."'),
  metadata: {
    '#withAnnotations':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotations(annotations): { metadata+: { annotations: annotations } },
    '#withAnnotationsMixin':: d.fn(help='"Annotations is an unstructured key value map stored with a resource that may be set by external tools to store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying objects. More info: http://kubernetes.io/docs/user-guide/annotations"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='annotations', type=d.T.object)]),
    withAnnotationsMixin(annotations): { metadata+: { annotations+: annotations } },
    '#withClusterName':: d.fn(help='"The name of the cluster which the object belongs to. This is used to distinguish resources with same name and namespace in different clusters. This field is not set anywhere right now and apiserver is going to ignore it if set in create or update request."', args=[d.arg(name='clusterName', type=d.T.string)]),
    withClusterName(clusterName): { metadata+: { clusterName: clusterName } },
    '#withCreationTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='creationTimestamp', type=d.T.string)]),
    withCreationTimestamp(creationTimestamp): { metadata+: { creationTimestamp: creationTimestamp } },
    '#withDeletionGracePeriodSeconds':: d.fn(help='"Number of seconds allowed for this object to gracefully terminate before it will be removed from the system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."', args=[d.arg(name='deletionGracePeriodSeconds', type=d.T.integer)]),
    withDeletionGracePeriodSeconds(deletionGracePeriodSeconds): { metadata+: { deletionGracePeriodSeconds: deletionGracePeriodSeconds } },
    '#withDeletionTimestamp':: d.fn(help='"Time is a wrapper around time.Time which supports correct marshaling to YAML and JSON.  Wrappers are provided for many of the factory methods that the time package offers."', args=[d.arg(name='deletionTimestamp', type=d.T.string)]),
    withDeletionTimestamp(deletionTimestamp): { metadata+: { deletionTimestamp: deletionTimestamp } },
    '#withFinalizers':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizers(finalizers): { metadata+: { finalizers: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withFinalizersMixin':: d.fn(help='"Must be empty before the object is deleted from the registry. Each entry is an identifier for the responsible component that will remove the entry from the list. If the deletionTimestamp of the object is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is a shared field, any actor with permission can reorder it. If the finalizer list is processed in order, then this can lead to a situation in which the component responsible for the first finalizer in the list is waiting for a signal (field value, external system, or other) produced by a component responsible for a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to order amongst themselves and are not vulnerable to ordering changes in the list."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='finalizers', type=d.T.array)]),
    withFinalizersMixin(finalizers): { metadata+: { finalizers+: if std.isArray(v=finalizers) then finalizers else [finalizers] } },
    '#withGenerateName':: d.fn(help='"GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field has not been provided. If this field is used, the name returned to the client will be different than the name passed. This value will also be combined with a unique suffix. The provided value has the same validation rules as the Name field, and may be truncated by the length of the suffix required to make the value unique on the server.\\n\\nIf this field is specified and the generated name exists, the server will NOT return a 409 - instead, it will either return 201 Created or 500 with Reason ServerTimeout indicating a unique name could not be found in the time allotted, and the client should retry (optionally after the time indicated in the Retry-After header).\\n\\nApplied only if Name is not specified. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"', args=[d.arg(name='generateName', type=d.T.string)]),
    withGenerateName(generateName): { metadata+: { generateName: generateName } },
    '#withGeneration':: d.fn(help='"A sequence number representing a specific generation of the desired state. Populated by the system. Read-only."', args=[d.arg(name='generation', type=d.T.integer)]),
    withGeneration(generation): { metadata+: { generation: generation } },
    '#withLabels':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"', args=[d.arg(name='labels', type=d.T.object)]),
    withLabels(labels): { metadata+: { labels: labels } },
    '#withLabelsMixin':: d.fn(help='"Map of string keys and values that can be used to organize and categorize (scope and select) objects. May match selectors of replication controllers and services. More info: http://kubernetes.io/docs/user-guide/labels"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='labels', type=d.T.object)]),
    withLabelsMixin(labels): { metadata+: { labels+: labels } },
    '#withName':: d.fn(help='"Name must be unique within a namespace. Is required when creating resources, although some resources may allow a client to request the generation of an appropriate name automatically. Name is primarily intended for creation idempotence and configuration definition. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/identifiers#names"', args=[d.arg(name='name', type=d.T.string)]),
    withName(name): { metadata+: { name: name } },
    '#withNamespace':: d.fn(help='"Namespace defines the space within which each name must be unique. An empty namespace is equivalent to the \\"default\\" namespace, but \\"default\\" is the canonical representation. Not all objects are required to be scoped to a namespace - the value of this field for those objects will be empty.\\n\\nMust be a DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"', args=[d.arg(name='namespace', type=d.T.string)]),
    withNamespace(namespace): { metadata+: { namespace: namespace } },
    '#withOwnerReferences':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferences(ownerReferences): { metadata+: { ownerReferences: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withOwnerReferencesMixin':: d.fn(help='"List of objects depended by this object. If ALL objects in the list have been deleted, this object will be garbage collected. If this object is managed by a controller, then an entry in this list will point to this controller, with the controller field set to true. There cannot be more than one managing controller."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ownerReferences', type=d.T.array)]),
    withOwnerReferencesMixin(ownerReferences): { metadata+: { ownerReferences+: if std.isArray(v=ownerReferences) then ownerReferences else [ownerReferences] } },
    '#withResourceVersion':: d.fn(help='"An opaque value that represents the internal version of this object that can be used by clients to determine when objects have changed. May be used for optimistic concurrency, change detection, and the watch operation on a resource or set of resources. Clients must treat these values as opaque and passed unmodified back to the server. They may only be valid for a particular resource or set of resources.\\n\\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"', args=[d.arg(name='resourceVersion', type=d.T.string)]),
    withResourceVersion(resourceVersion): { metadata+: { resourceVersion: resourceVersion } },
    '#withSelfLink':: d.fn(help='"SelfLink is a URL representing this object. Populated by the system. Read-only.\\n\\nDEPRECATED Kubernetes will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."', args=[d.arg(name='selfLink', type=d.T.string)]),
    withSelfLink(selfLink): { metadata+: { selfLink: selfLink } },
    '#withUid':: d.fn(help='"UID is the unique in time and space value for this object. It is typically generated by the server on successful creation of a resource and is not allowed to change on PUT operations.\\n\\nPopulated by the system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"', args=[d.arg(name='uid', type=d.T.string)]),
    withUid(uid): { metadata+: { uid: uid } },
  },
  '#new':: d.fn(help='new returns an instance of Table', args=[d.arg(name='name', type=d.T.string)]),
  new(name): {
    apiVersion: 'dynamodb.aws.upbound.io/v1beta1',
    kind: 'Table',
  } + self.metadata.withName(name=name) + self.metadata.withAnnotations(annotations={
    'tanka.dev/namespaced': 'false',
  }),
  '#spec':: d.obj(help='"TableSpec defines the desired state of Table"'),
  spec: {
    '#forProvider':: d.obj(help=''),
    forProvider: {
      '#attribute':: d.obj(help='"Set of nested attribute definitions. Only required for hash_key and range_key attributes. See below."'),
      attribute: {
        '#withName':: d.fn(help='"Name of the attribute"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withType':: d.fn(help='"Attribute type. Valid values are S (string), N (number), B (binary)."', args=[d.arg(name='type', type=d.T.string)]),
        withType(type): { type: type },
      },
      '#globalSecondaryIndex':: d.obj(help='"Describe a GSI for the table; subject to the normal limits on the number of GSIs, projected attributes, etc. See below."'),
      globalSecondaryIndex: {
        '#onDemandThroughput':: d.obj(help='"Sets the maximum number of read and write units for the specified on-demand index. See below."'),
        onDemandThroughput: {
          '#withMaxReadRequestUnits':: d.fn(help='"Maximum number of read request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxReadRequestUnits', type=d.T.number)]),
          withMaxReadRequestUnits(maxReadRequestUnits): { maxReadRequestUnits: maxReadRequestUnits },
          '#withMaxWriteRequestUnits':: d.fn(help='"Maximum number of write request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxWriteRequestUnits', type=d.T.number)]),
          withMaxWriteRequestUnits(maxWriteRequestUnits): { maxWriteRequestUnits: maxWriteRequestUnits },
        },
        '#warmThroughput':: d.obj(help='"Sets the number of warm read and write units for this index. See below."'),
        warmThroughput: {
          '#withReadUnitsPerSecond':: d.fn(help='"Number of read operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 12000 (default)."', args=[d.arg(name='readUnitsPerSecond', type=d.T.number)]),
          withReadUnitsPerSecond(readUnitsPerSecond): { readUnitsPerSecond: readUnitsPerSecond },
          '#withWriteUnitsPerSecond':: d.fn(help='"Number of write operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 4000 (default)."', args=[d.arg(name='writeUnitsPerSecond', type=d.T.number)]),
          withWriteUnitsPerSecond(writeUnitsPerSecond): { writeUnitsPerSecond: writeUnitsPerSecond },
        },
        '#withHashKey':: d.fn(help='"Name of the hash key in the index; must be defined as an attribute in the resource."', args=[d.arg(name='hashKey', type=d.T.string)]),
        withHashKey(hashKey): { hashKey: hashKey },
        '#withName':: d.fn(help='"Name of the index."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withNonKeyAttributes':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributes(nonKeyAttributes): { nonKeyAttributes: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withNonKeyAttributesMixin':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributesMixin(nonKeyAttributes): { nonKeyAttributes+: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withOnDemandThroughput':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand index. See below."', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
        withOnDemandThroughput(onDemandThroughput): { onDemandThroughput: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] },
        '#withOnDemandThroughputMixin':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand index. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
        withOnDemandThroughputMixin(onDemandThroughput): { onDemandThroughput+: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] },
        '#withProjectionType':: d.fn(help='"One of ALL, INCLUDE or KEYS_ONLY where ALL projects every attribute into the index, KEYS_ONLY projects  into the index only the table and index hash_key and sort_key attributes ,  INCLUDE projects into the index all of the attributes that are defined in non_key_attributes in addition to the attributes that thatKEYS_ONLY project."', args=[d.arg(name='projectionType', type=d.T.string)]),
        withProjectionType(projectionType): { projectionType: projectionType },
        '#withRangeKey':: d.fn(help='"Name of the range key; must be defined"', args=[d.arg(name='rangeKey', type=d.T.string)]),
        withRangeKey(rangeKey): { rangeKey: rangeKey },
        '#withReadCapacity':: d.fn(help='"Number of read units for this index. Must be set if billing_mode is set to PROVISIONED."', args=[d.arg(name='readCapacity', type=d.T.number)]),
        withReadCapacity(readCapacity): { readCapacity: readCapacity },
        '#withWarmThroughput':: d.fn(help='"Sets the number of warm read and write units for this index. See below."', args=[d.arg(name='warmThroughput', type=d.T.array)]),
        withWarmThroughput(warmThroughput): { warmThroughput: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] },
        '#withWarmThroughputMixin':: d.fn(help='"Sets the number of warm read and write units for this index. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='warmThroughput', type=d.T.array)]),
        withWarmThroughputMixin(warmThroughput): { warmThroughput+: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] },
        '#withWriteCapacity':: d.fn(help='"Number of write units for this index. Must be set if billing_mode is set to PROVISIONED."', args=[d.arg(name='writeCapacity', type=d.T.number)]),
        withWriteCapacity(writeCapacity): { writeCapacity: writeCapacity },
      },
      '#importTable':: d.obj(help='"Import Amazon S3 data into a new table. See below."'),
      importTable: {
        '#inputFormatOptions':: d.obj(help='"Describe the format options for the data that was imported into the target table.\\nThere is one value, csv.\\nSee below."'),
        inputFormatOptions: {
          '#csv':: d.obj(help='"This block contains the processing options for the CSV file being imported:"'),
          csv: {
            '#withDelimiter':: d.fn(help='"The delimiter used for separating items in the CSV file being imported."', args=[d.arg(name='delimiter', type=d.T.string)]),
            withDelimiter(delimiter): { delimiter: delimiter },
            '#withHeaderList':: d.fn(help='"List of the headers used to specify a common header for all source CSV files being imported."', args=[d.arg(name='headerList', type=d.T.array)]),
            withHeaderList(headerList): { headerList: if std.isArray(v=headerList) then headerList else [headerList] },
            '#withHeaderListMixin':: d.fn(help='"List of the headers used to specify a common header for all source CSV files being imported."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headerList', type=d.T.array)]),
            withHeaderListMixin(headerList): { headerList+: if std.isArray(v=headerList) then headerList else [headerList] },
          },
          '#withCsv':: d.fn(help='"This block contains the processing options for the CSV file being imported:"', args=[d.arg(name='csv', type=d.T.array)]),
          withCsv(csv): { csv: if std.isArray(v=csv) then csv else [csv] },
          '#withCsvMixin':: d.fn(help='"This block contains the processing options for the CSV file being imported:"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='csv', type=d.T.array)]),
          withCsvMixin(csv): { csv+: if std.isArray(v=csv) then csv else [csv] },
        },
        '#s3BucketSource':: d.obj(help='"Values for the S3 bucket the source file is imported from.\\nSee below."'),
        s3BucketSource: {
          '#withBucket':: d.fn(help='"The S3 bucket that is being imported from."', args=[d.arg(name='bucket', type=d.T.string)]),
          withBucket(bucket): { bucket: bucket },
          '#withBucketOwner':: d.fn(help='"The account number of the S3 bucket that is being imported from."', args=[d.arg(name='bucketOwner', type=d.T.string)]),
          withBucketOwner(bucketOwner): { bucketOwner: bucketOwner },
          '#withKeyPrefix':: d.fn(help='"The key prefix shared by all S3 Objects that are being imported."', args=[d.arg(name='keyPrefix', type=d.T.string)]),
          withKeyPrefix(keyPrefix): { keyPrefix: keyPrefix },
        },
        '#withInputCompressionType':: d.fn(help='"Type of compression to be used on the input coming from the imported table.\\nValid values are GZIP, ZSTD and NONE."', args=[d.arg(name='inputCompressionType', type=d.T.string)]),
        withInputCompressionType(inputCompressionType): { inputCompressionType: inputCompressionType },
        '#withInputFormat':: d.fn(help='"The format of the source data.\\nValid values are CSV, DYNAMODB_JSON, and ION."', args=[d.arg(name='inputFormat', type=d.T.string)]),
        withInputFormat(inputFormat): { inputFormat: inputFormat },
        '#withInputFormatOptions':: d.fn(help='"Describe the format options for the data that was imported into the target table.\\nThere is one value, csv.\\nSee below."', args=[d.arg(name='inputFormatOptions', type=d.T.array)]),
        withInputFormatOptions(inputFormatOptions): { inputFormatOptions: if std.isArray(v=inputFormatOptions) then inputFormatOptions else [inputFormatOptions] },
        '#withInputFormatOptionsMixin':: d.fn(help='"Describe the format options for the data that was imported into the target table.\\nThere is one value, csv.\\nSee below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='inputFormatOptions', type=d.T.array)]),
        withInputFormatOptionsMixin(inputFormatOptions): { inputFormatOptions+: if std.isArray(v=inputFormatOptions) then inputFormatOptions else [inputFormatOptions] },
        '#withS3BucketSource':: d.fn(help='"Values for the S3 bucket the source file is imported from.\\nSee below."', args=[d.arg(name='s3BucketSource', type=d.T.array)]),
        withS3BucketSource(s3BucketSource): { s3BucketSource: if std.isArray(v=s3BucketSource) then s3BucketSource else [s3BucketSource] },
        '#withS3BucketSourceMixin':: d.fn(help='"Values for the S3 bucket the source file is imported from.\\nSee below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='s3BucketSource', type=d.T.array)]),
        withS3BucketSourceMixin(s3BucketSource): { s3BucketSource+: if std.isArray(v=s3BucketSource) then s3BucketSource else [s3BucketSource] },
      },
      '#localSecondaryIndex':: d.obj(help='"Describe an LSI on the table; these can only be allocated at creation so you cannot change this definition after you have created the resource. See below."'),
      localSecondaryIndex: {
        '#withName':: d.fn(help='"Name of the index"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withNonKeyAttributes':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributes(nonKeyAttributes): { nonKeyAttributes: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withNonKeyAttributesMixin':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributesMixin(nonKeyAttributes): { nonKeyAttributes+: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withProjectionType':: d.fn(help='"One of ALL, INCLUDE or KEYS_ONLY where ALL projects every attribute into the index, KEYS_ONLY projects  into the index only the table and index hash_key and sort_key attributes ,  INCLUDE projects into the index all of the attributes that are defined in non_key_attributes in addition to the attributes that thatKEYS_ONLY project."', args=[d.arg(name='projectionType', type=d.T.string)]),
        withProjectionType(projectionType): { projectionType: projectionType },
        '#withRangeKey':: d.fn(help='"Name of the range key."', args=[d.arg(name='rangeKey', type=d.T.string)]),
        withRangeKey(rangeKey): { rangeKey: rangeKey },
      },
      '#onDemandThroughput':: d.obj(help='"Sets the maximum number of read and write units for the specified on-demand table. See below."'),
      onDemandThroughput: {
        '#withMaxReadRequestUnits':: d.fn(help='"Maximum number of read request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxReadRequestUnits', type=d.T.number)]),
        withMaxReadRequestUnits(maxReadRequestUnits): { maxReadRequestUnits: maxReadRequestUnits },
        '#withMaxWriteRequestUnits':: d.fn(help='"Maximum number of write request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxWriteRequestUnits', type=d.T.number)]),
        withMaxWriteRequestUnits(maxWriteRequestUnits): { maxWriteRequestUnits: maxWriteRequestUnits },
      },
      '#pointInTimeRecovery':: d.obj(help='"Enable point-in-time recovery options. See below."'),
      pointInTimeRecovery: {
        '#withEnabled':: d.fn(help='"Whether to enable point-in-time recovery. It can take 10 minutes to enable for new tables. If the point_in_time_recovery block is not provided, this defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
        '#withRecoveryPeriodInDays':: d.fn(help='"Number of preceding days for which continuous backups are taken and maintained. Default is 35."', args=[d.arg(name='recoveryPeriodInDays', type=d.T.number)]),
        withRecoveryPeriodInDays(recoveryPeriodInDays): { recoveryPeriodInDays: recoveryPeriodInDays },
      },
      '#replica':: d.obj(help='"Configuration block(s) with DynamoDB Global Tables V2 (version 2019.11.21) replication configurations. See below."'),
      replica: {
        '#withConsistencyMode':: d.fn(help='"Whether this global table will be using STRONG consistency mode or EVENTUAL consistency mode. Default value is EVENTUAL."', args=[d.arg(name='consistencyMode', type=d.T.string)]),
        withConsistencyMode(consistencyMode): { consistencyMode: consistencyMode },
        '#withDeletionProtectionEnabled':: d.fn(help='"Whether deletion protection is enabled (true) or disabled (false) on the replica. Default is false."', args=[d.arg(name='deletionProtectionEnabled', type=d.T.boolean)]),
        withDeletionProtectionEnabled(deletionProtectionEnabled): { deletionProtectionEnabled: deletionProtectionEnabled },
        '#withKmsKeyArn':: d.fn(help='"ARN of the CMK that should be used for the AWS KMS encryption.\\nThis argument should only be used if the key is different from the default KMS-managed DynamoDB key, alias/aws/dynamodb.\\nNote: This attribute will not be populated with the ARN of default keys.\\nNote: Changing this value will recreate the replica."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
        withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
        '#withPointInTimeRecovery':: d.fn(help='"Whether to enable Point In Time Recovery for the replica. Default is false."', args=[d.arg(name='pointInTimeRecovery', type=d.T.boolean)]),
        withPointInTimeRecovery(pointInTimeRecovery): { pointInTimeRecovery: pointInTimeRecovery },
        '#withPropagateTags':: d.fn(help="\"Whether to propagate the global table's tags to a replica.\\nDefault is false.\\nChanges to tags only move in one direction: from global (source) to replica.\\nTag drift on a replica will not trigger an update.\\nTag changes on the global table are propagated to replicas.\\nChanging from true to false on a subsequent apply leaves replica tags as-is and no longer manages them.\"", args=[d.arg(name='propagateTags', type=d.T.boolean)]),
        withPropagateTags(propagateTags): { propagateTags: propagateTags },
        '#withRegionName':: d.fn(help='"Region name of the replica."', args=[d.arg(name='regionName', type=d.T.string)]),
        withRegionName(regionName): { regionName: regionName },
      },
      '#serverSideEncryption':: d.obj(help="\"Encryption at rest options. AWS DynamoDB tables are automatically encrypted at rest with an AWS-owned Customer Master Key if this argument isn't specified. Must be supplied for cross-region restores. See below.\""),
      serverSideEncryption: {
        '#withEnabled':: d.fn(help='"Whether or not to enable encryption at rest using an AWS managed KMS customer master key (CMK). If enabled is false then server-side encryption is set to AWS-owned key (shown as DEFAULT in the AWS console). Potentially confusingly, if enabled is true and no kms_key_arn is specified then server-side encryption is set to the default KMS-managed key (shown as KMS in the AWS console). The AWS KMS documentation explains the difference between AWS-owned and KMS-managed keys."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
        '#withKmsKeyArn':: d.fn(help='"ARN of the CMK that should be used for the AWS KMS encryption. This argument should only be used if the key is different from the default KMS-managed DynamoDB key, alias/aws/dynamodb. Note: This attribute will not be populated with the ARN of default keys."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
        withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
      },
      '#ttl':: d.obj(help='"Configuration block for TTL. See below."'),
      ttl: {
        '#withAttributeName':: d.fn(help='"Name of the table attribute to store the TTL timestamp in.\\nRequired if enabled is true, must not be set otherwise."', args=[d.arg(name='attributeName', type=d.T.string)]),
        withAttributeName(attributeName): { attributeName: attributeName },
        '#withEnabled':: d.fn(help='"Whether TTL is enabled.\\nDefault value is false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
      },
      '#warmThroughput':: d.obj(help='"Sets the number of warm read and write units for the specified table. See below."'),
      warmThroughput: {
        '#withReadUnitsPerSecond':: d.fn(help='"Number of read operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 12000 (default)."', args=[d.arg(name='readUnitsPerSecond', type=d.T.number)]),
        withReadUnitsPerSecond(readUnitsPerSecond): { readUnitsPerSecond: readUnitsPerSecond },
        '#withWriteUnitsPerSecond':: d.fn(help='"Number of write operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 4000 (default)."', args=[d.arg(name='writeUnitsPerSecond', type=d.T.number)]),
        withWriteUnitsPerSecond(writeUnitsPerSecond): { writeUnitsPerSecond: writeUnitsPerSecond },
      },
      '#withAttribute':: d.fn(help='"Set of nested attribute definitions. Only required for hash_key and range_key attributes. See below."', args=[d.arg(name='attribute', type=d.T.array)]),
      withAttribute(attribute): { spec+: { forProvider+: { attribute: if std.isArray(v=attribute) then attribute else [attribute] } } },
      '#withAttributeMixin':: d.fn(help='"Set of nested attribute definitions. Only required for hash_key and range_key attributes. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='attribute', type=d.T.array)]),
      withAttributeMixin(attribute): { spec+: { forProvider+: { attribute+: if std.isArray(v=attribute) then attribute else [attribute] } } },
      '#withBillingMode':: d.fn(help='"Controls how you are charged for read and write throughput and how you manage capacity. The valid values are PROVISIONED and PAY_PER_REQUEST. Defaults to PROVISIONED."', args=[d.arg(name='billingMode', type=d.T.string)]),
      withBillingMode(billingMode): { spec+: { forProvider+: { billingMode: billingMode } } },
      '#withDeletionProtectionEnabled':: d.fn(help='"Enables deletion protection for table. Defaults to false."', args=[d.arg(name='deletionProtectionEnabled', type=d.T.boolean)]),
      withDeletionProtectionEnabled(deletionProtectionEnabled): { spec+: { forProvider+: { deletionProtectionEnabled: deletionProtectionEnabled } } },
      '#withGlobalSecondaryIndex':: d.fn(help='"Describe a GSI for the table; subject to the normal limits on the number of GSIs, projected attributes, etc. See below."', args=[d.arg(name='globalSecondaryIndex', type=d.T.array)]),
      withGlobalSecondaryIndex(globalSecondaryIndex): { spec+: { forProvider+: { globalSecondaryIndex: if std.isArray(v=globalSecondaryIndex) then globalSecondaryIndex else [globalSecondaryIndex] } } },
      '#withGlobalSecondaryIndexMixin':: d.fn(help='"Describe a GSI for the table; subject to the normal limits on the number of GSIs, projected attributes, etc. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='globalSecondaryIndex', type=d.T.array)]),
      withGlobalSecondaryIndexMixin(globalSecondaryIndex): { spec+: { forProvider+: { globalSecondaryIndex+: if std.isArray(v=globalSecondaryIndex) then globalSecondaryIndex else [globalSecondaryIndex] } } },
      '#withHashKey':: d.fn(help='"Attribute to use as the hash (partition) key. Must also be defined as an attribute. See below."', args=[d.arg(name='hashKey', type=d.T.string)]),
      withHashKey(hashKey): { spec+: { forProvider+: { hashKey: hashKey } } },
      '#withImportTable':: d.fn(help='"Import Amazon S3 data into a new table. See below."', args=[d.arg(name='importTable', type=d.T.array)]),
      withImportTable(importTable): { spec+: { forProvider+: { importTable: if std.isArray(v=importTable) then importTable else [importTable] } } },
      '#withImportTableMixin':: d.fn(help='"Import Amazon S3 data into a new table. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='importTable', type=d.T.array)]),
      withImportTableMixin(importTable): { spec+: { forProvider+: { importTable+: if std.isArray(v=importTable) then importTable else [importTable] } } },
      '#withLocalSecondaryIndex':: d.fn(help='"Describe an LSI on the table; these can only be allocated at creation so you cannot change this definition after you have created the resource. See below."', args=[d.arg(name='localSecondaryIndex', type=d.T.array)]),
      withLocalSecondaryIndex(localSecondaryIndex): { spec+: { forProvider+: { localSecondaryIndex: if std.isArray(v=localSecondaryIndex) then localSecondaryIndex else [localSecondaryIndex] } } },
      '#withLocalSecondaryIndexMixin':: d.fn(help='"Describe an LSI on the table; these can only be allocated at creation so you cannot change this definition after you have created the resource. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='localSecondaryIndex', type=d.T.array)]),
      withLocalSecondaryIndexMixin(localSecondaryIndex): { spec+: { forProvider+: { localSecondaryIndex+: if std.isArray(v=localSecondaryIndex) then localSecondaryIndex else [localSecondaryIndex] } } },
      '#withOnDemandThroughput':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand table. See below."', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
      withOnDemandThroughput(onDemandThroughput): { spec+: { forProvider+: { onDemandThroughput: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] } } },
      '#withOnDemandThroughputMixin':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand table. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
      withOnDemandThroughputMixin(onDemandThroughput): { spec+: { forProvider+: { onDemandThroughput+: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] } } },
      '#withPointInTimeRecovery':: d.fn(help='"Enable point-in-time recovery options. See below."', args=[d.arg(name='pointInTimeRecovery', type=d.T.array)]),
      withPointInTimeRecovery(pointInTimeRecovery): { spec+: { forProvider+: { pointInTimeRecovery: if std.isArray(v=pointInTimeRecovery) then pointInTimeRecovery else [pointInTimeRecovery] } } },
      '#withPointInTimeRecoveryMixin':: d.fn(help='"Enable point-in-time recovery options. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pointInTimeRecovery', type=d.T.array)]),
      withPointInTimeRecoveryMixin(pointInTimeRecovery): { spec+: { forProvider+: { pointInTimeRecovery+: if std.isArray(v=pointInTimeRecovery) then pointInTimeRecovery else [pointInTimeRecovery] } } },
      '#withRangeKey':: d.fn(help='"Attribute to use as the range (sort) key. Must also be defined as an attribute, see below."', args=[d.arg(name='rangeKey', type=d.T.string)]),
      withRangeKey(rangeKey): { spec+: { forProvider+: { rangeKey: rangeKey } } },
      '#withReadCapacity':: d.fn(help='"Number of read units for this table. If the billing_mode is PROVISIONED, this field is required."', args=[d.arg(name='readCapacity', type=d.T.number)]),
      withReadCapacity(readCapacity): { spec+: { forProvider+: { readCapacity: readCapacity } } },
      '#withRegion':: d.fn(help="\"Region where this resource will be managed. Defaults to the Region set in the provider configuration.\\nRegion is the region you'd like your resource to be created in.\"", args=[d.arg(name='region', type=d.T.string)]),
      withRegion(region): { spec+: { forProvider+: { region: region } } },
      '#withReplica':: d.fn(help='"Configuration block(s) with DynamoDB Global Tables V2 (version 2019.11.21) replication configurations. See below."', args=[d.arg(name='replica', type=d.T.array)]),
      withReplica(replica): { spec+: { forProvider+: { replica: if std.isArray(v=replica) then replica else [replica] } } },
      '#withReplicaMixin':: d.fn(help='"Configuration block(s) with DynamoDB Global Tables V2 (version 2019.11.21) replication configurations. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='replica', type=d.T.array)]),
      withReplicaMixin(replica): { spec+: { forProvider+: { replica+: if std.isArray(v=replica) then replica else [replica] } } },
      '#withRestoreDateTime':: d.fn(help='"Time of the point-in-time recovery point to restore."', args=[d.arg(name='restoreDateTime', type=d.T.string)]),
      withRestoreDateTime(restoreDateTime): { spec+: { forProvider+: { restoreDateTime: restoreDateTime } } },
      '#withRestoreSourceName':: d.fn(help='"Name of the table to restore. Must match the name of an existing table."', args=[d.arg(name='restoreSourceName', type=d.T.string)]),
      withRestoreSourceName(restoreSourceName): { spec+: { forProvider+: { restoreSourceName: restoreSourceName } } },
      '#withRestoreSourceTableArn':: d.fn(help='"ARN of the source table to restore. Must be supplied for cross-region restores."', args=[d.arg(name='restoreSourceTableArn', type=d.T.string)]),
      withRestoreSourceTableArn(restoreSourceTableArn): { spec+: { forProvider+: { restoreSourceTableArn: restoreSourceTableArn } } },
      '#withRestoreToLatestTime':: d.fn(help='"If set, restores table to the most recent point-in-time recovery point."', args=[d.arg(name='restoreToLatestTime', type=d.T.boolean)]),
      withRestoreToLatestTime(restoreToLatestTime): { spec+: { forProvider+: { restoreToLatestTime: restoreToLatestTime } } },
      '#withServerSideEncryption':: d.fn(help="\"Encryption at rest options. AWS DynamoDB tables are automatically encrypted at rest with an AWS-owned Customer Master Key if this argument isn't specified. Must be supplied for cross-region restores. See below.\"", args=[d.arg(name='serverSideEncryption', type=d.T.array)]),
      withServerSideEncryption(serverSideEncryption): { spec+: { forProvider+: { serverSideEncryption: if std.isArray(v=serverSideEncryption) then serverSideEncryption else [serverSideEncryption] } } },
      '#withServerSideEncryptionMixin':: d.fn(help="\"Encryption at rest options. AWS DynamoDB tables are automatically encrypted at rest with an AWS-owned Customer Master Key if this argument isn't specified. Must be supplied for cross-region restores. See below.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='serverSideEncryption', type=d.T.array)]),
      withServerSideEncryptionMixin(serverSideEncryption): { spec+: { forProvider+: { serverSideEncryption+: if std.isArray(v=serverSideEncryption) then serverSideEncryption else [serverSideEncryption] } } },
      '#withStreamEnabled':: d.fn(help='"Whether Streams are enabled."', args=[d.arg(name='streamEnabled', type=d.T.boolean)]),
      withStreamEnabled(streamEnabled): { spec+: { forProvider+: { streamEnabled: streamEnabled } } },
      '#withStreamViewType':: d.fn(help="\"When an item in the table is modified, StreamViewType determines what information is written to the table's stream. Valid values are KEYS_ONLY, NEW_IMAGE, OLD_IMAGE, NEW_AND_OLD_IMAGES.\"", args=[d.arg(name='streamViewType', type=d.T.string)]),
      withStreamViewType(streamViewType): { spec+: { forProvider+: { streamViewType: streamViewType } } },
      '#withTableClass':: d.fn(help='"Storage class of the table.\\nValid values are STANDARD and STANDARD_INFREQUENT_ACCESS.\\nDefault value is STANDARD."', args=[d.arg(name='tableClass', type=d.T.string)]),
      withTableClass(tableClass): { spec+: { forProvider+: { tableClass: tableClass } } },
      '#withTags':: d.fn(help='"Key-value map of resource tags."', args=[d.arg(name='tags', type=d.T.object)]),
      withTags(tags): { spec+: { forProvider+: { tags: tags } } },
      '#withTagsMixin':: d.fn(help='"Key-value map of resource tags."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
      withTagsMixin(tags): { spec+: { forProvider+: { tags+: tags } } },
      '#withTtl':: d.fn(help='"Configuration block for TTL. See below."', args=[d.arg(name='ttl', type=d.T.array)]),
      withTtl(ttl): { spec+: { forProvider+: { ttl: if std.isArray(v=ttl) then ttl else [ttl] } } },
      '#withTtlMixin':: d.fn(help='"Configuration block for TTL. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ttl', type=d.T.array)]),
      withTtlMixin(ttl): { spec+: { forProvider+: { ttl+: if std.isArray(v=ttl) then ttl else [ttl] } } },
      '#withWarmThroughput':: d.fn(help='"Sets the number of warm read and write units for the specified table. See below."', args=[d.arg(name='warmThroughput', type=d.T.array)]),
      withWarmThroughput(warmThroughput): { spec+: { forProvider+: { warmThroughput: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] } } },
      '#withWarmThroughputMixin':: d.fn(help='"Sets the number of warm read and write units for the specified table. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='warmThroughput', type=d.T.array)]),
      withWarmThroughputMixin(warmThroughput): { spec+: { forProvider+: { warmThroughput+: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] } } },
      '#withWriteCapacity':: d.fn(help='"Number of write units for this table. If the billing_mode is PROVISIONED, this field is required."', args=[d.arg(name='writeCapacity', type=d.T.number)]),
      withWriteCapacity(writeCapacity): { spec+: { forProvider+: { writeCapacity: writeCapacity } } },
    },
    '#initProvider':: d.obj(help='"THIS IS A BETA FIELD. It will be honored\\nunless the Management Policies feature flag is disabled.\\nInitProvider holds the same fields as ForProvider, with the exception\\nof Identifier and other resource reference fields. The fields that are\\nin InitProvider are merged into ForProvider when the resource is created.\\nThe same fields are also added to the terraform ignore_changes hook, to\\navoid updating them after creation. This is useful for fields that are\\nrequired on creation, but we do not desire to update them after creation,\\nfor example because of an external controller is managing them, like an\\nautoscaler."'),
    initProvider: {
      '#attribute':: d.obj(help='"Set of nested attribute definitions. Only required for hash_key and range_key attributes. See below."'),
      attribute: {
        '#withName':: d.fn(help='"Name of the attribute"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withType':: d.fn(help='"Attribute type. Valid values are S (string), N (number), B (binary)."', args=[d.arg(name='type', type=d.T.string)]),
        withType(type): { type: type },
      },
      '#globalSecondaryIndex':: d.obj(help='"Describe a GSI for the table; subject to the normal limits on the number of GSIs, projected attributes, etc. See below."'),
      globalSecondaryIndex: {
        '#onDemandThroughput':: d.obj(help='"Sets the maximum number of read and write units for the specified on-demand index. See below."'),
        onDemandThroughput: {
          '#withMaxReadRequestUnits':: d.fn(help='"Maximum number of read request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxReadRequestUnits', type=d.T.number)]),
          withMaxReadRequestUnits(maxReadRequestUnits): { maxReadRequestUnits: maxReadRequestUnits },
          '#withMaxWriteRequestUnits':: d.fn(help='"Maximum number of write request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxWriteRequestUnits', type=d.T.number)]),
          withMaxWriteRequestUnits(maxWriteRequestUnits): { maxWriteRequestUnits: maxWriteRequestUnits },
        },
        '#warmThroughput':: d.obj(help='"Sets the number of warm read and write units for this index. See below."'),
        warmThroughput: {
          '#withReadUnitsPerSecond':: d.fn(help='"Number of read operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 12000 (default)."', args=[d.arg(name='readUnitsPerSecond', type=d.T.number)]),
          withReadUnitsPerSecond(readUnitsPerSecond): { readUnitsPerSecond: readUnitsPerSecond },
          '#withWriteUnitsPerSecond':: d.fn(help='"Number of write operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 4000 (default)."', args=[d.arg(name='writeUnitsPerSecond', type=d.T.number)]),
          withWriteUnitsPerSecond(writeUnitsPerSecond): { writeUnitsPerSecond: writeUnitsPerSecond },
        },
        '#withHashKey':: d.fn(help='"Name of the hash key in the index; must be defined as an attribute in the resource."', args=[d.arg(name='hashKey', type=d.T.string)]),
        withHashKey(hashKey): { hashKey: hashKey },
        '#withName':: d.fn(help='"Name of the index."', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withNonKeyAttributes':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributes(nonKeyAttributes): { nonKeyAttributes: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withNonKeyAttributesMixin':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributesMixin(nonKeyAttributes): { nonKeyAttributes+: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withOnDemandThroughput':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand index. See below."', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
        withOnDemandThroughput(onDemandThroughput): { onDemandThroughput: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] },
        '#withOnDemandThroughputMixin':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand index. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
        withOnDemandThroughputMixin(onDemandThroughput): { onDemandThroughput+: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] },
        '#withProjectionType':: d.fn(help='"One of ALL, INCLUDE or KEYS_ONLY where ALL projects every attribute into the index, KEYS_ONLY projects  into the index only the table and index hash_key and sort_key attributes ,  INCLUDE projects into the index all of the attributes that are defined in non_key_attributes in addition to the attributes that thatKEYS_ONLY project."', args=[d.arg(name='projectionType', type=d.T.string)]),
        withProjectionType(projectionType): { projectionType: projectionType },
        '#withRangeKey':: d.fn(help='"Name of the range key; must be defined"', args=[d.arg(name='rangeKey', type=d.T.string)]),
        withRangeKey(rangeKey): { rangeKey: rangeKey },
        '#withReadCapacity':: d.fn(help='"Number of read units for this index. Must be set if billing_mode is set to PROVISIONED."', args=[d.arg(name='readCapacity', type=d.T.number)]),
        withReadCapacity(readCapacity): { readCapacity: readCapacity },
        '#withWarmThroughput':: d.fn(help='"Sets the number of warm read and write units for this index. See below."', args=[d.arg(name='warmThroughput', type=d.T.array)]),
        withWarmThroughput(warmThroughput): { warmThroughput: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] },
        '#withWarmThroughputMixin':: d.fn(help='"Sets the number of warm read and write units for this index. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='warmThroughput', type=d.T.array)]),
        withWarmThroughputMixin(warmThroughput): { warmThroughput+: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] },
        '#withWriteCapacity':: d.fn(help='"Number of write units for this index. Must be set if billing_mode is set to PROVISIONED."', args=[d.arg(name='writeCapacity', type=d.T.number)]),
        withWriteCapacity(writeCapacity): { writeCapacity: writeCapacity },
      },
      '#importTable':: d.obj(help='"Import Amazon S3 data into a new table. See below."'),
      importTable: {
        '#inputFormatOptions':: d.obj(help='"Describe the format options for the data that was imported into the target table.\\nThere is one value, csv.\\nSee below."'),
        inputFormatOptions: {
          '#csv':: d.obj(help='"This block contains the processing options for the CSV file being imported:"'),
          csv: {
            '#withDelimiter':: d.fn(help='"The delimiter used for separating items in the CSV file being imported."', args=[d.arg(name='delimiter', type=d.T.string)]),
            withDelimiter(delimiter): { delimiter: delimiter },
            '#withHeaderList':: d.fn(help='"List of the headers used to specify a common header for all source CSV files being imported."', args=[d.arg(name='headerList', type=d.T.array)]),
            withHeaderList(headerList): { headerList: if std.isArray(v=headerList) then headerList else [headerList] },
            '#withHeaderListMixin':: d.fn(help='"List of the headers used to specify a common header for all source CSV files being imported."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='headerList', type=d.T.array)]),
            withHeaderListMixin(headerList): { headerList+: if std.isArray(v=headerList) then headerList else [headerList] },
          },
          '#withCsv':: d.fn(help='"This block contains the processing options for the CSV file being imported:"', args=[d.arg(name='csv', type=d.T.array)]),
          withCsv(csv): { csv: if std.isArray(v=csv) then csv else [csv] },
          '#withCsvMixin':: d.fn(help='"This block contains the processing options for the CSV file being imported:"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='csv', type=d.T.array)]),
          withCsvMixin(csv): { csv+: if std.isArray(v=csv) then csv else [csv] },
        },
        '#s3BucketSource':: d.obj(help='"Values for the S3 bucket the source file is imported from.\\nSee below."'),
        s3BucketSource: {
          '#withBucket':: d.fn(help='"The S3 bucket that is being imported from."', args=[d.arg(name='bucket', type=d.T.string)]),
          withBucket(bucket): { bucket: bucket },
          '#withBucketOwner':: d.fn(help='"The account number of the S3 bucket that is being imported from."', args=[d.arg(name='bucketOwner', type=d.T.string)]),
          withBucketOwner(bucketOwner): { bucketOwner: bucketOwner },
          '#withKeyPrefix':: d.fn(help='"The key prefix shared by all S3 Objects that are being imported."', args=[d.arg(name='keyPrefix', type=d.T.string)]),
          withKeyPrefix(keyPrefix): { keyPrefix: keyPrefix },
        },
        '#withInputCompressionType':: d.fn(help='"Type of compression to be used on the input coming from the imported table.\\nValid values are GZIP, ZSTD and NONE."', args=[d.arg(name='inputCompressionType', type=d.T.string)]),
        withInputCompressionType(inputCompressionType): { inputCompressionType: inputCompressionType },
        '#withInputFormat':: d.fn(help='"The format of the source data.\\nValid values are CSV, DYNAMODB_JSON, and ION."', args=[d.arg(name='inputFormat', type=d.T.string)]),
        withInputFormat(inputFormat): { inputFormat: inputFormat },
        '#withInputFormatOptions':: d.fn(help='"Describe the format options for the data that was imported into the target table.\\nThere is one value, csv.\\nSee below."', args=[d.arg(name='inputFormatOptions', type=d.T.array)]),
        withInputFormatOptions(inputFormatOptions): { inputFormatOptions: if std.isArray(v=inputFormatOptions) then inputFormatOptions else [inputFormatOptions] },
        '#withInputFormatOptionsMixin':: d.fn(help='"Describe the format options for the data that was imported into the target table.\\nThere is one value, csv.\\nSee below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='inputFormatOptions', type=d.T.array)]),
        withInputFormatOptionsMixin(inputFormatOptions): { inputFormatOptions+: if std.isArray(v=inputFormatOptions) then inputFormatOptions else [inputFormatOptions] },
        '#withS3BucketSource':: d.fn(help='"Values for the S3 bucket the source file is imported from.\\nSee below."', args=[d.arg(name='s3BucketSource', type=d.T.array)]),
        withS3BucketSource(s3BucketSource): { s3BucketSource: if std.isArray(v=s3BucketSource) then s3BucketSource else [s3BucketSource] },
        '#withS3BucketSourceMixin':: d.fn(help='"Values for the S3 bucket the source file is imported from.\\nSee below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='s3BucketSource', type=d.T.array)]),
        withS3BucketSourceMixin(s3BucketSource): { s3BucketSource+: if std.isArray(v=s3BucketSource) then s3BucketSource else [s3BucketSource] },
      },
      '#localSecondaryIndex':: d.obj(help='"Describe an LSI on the table; these can only be allocated at creation so you cannot change this definition after you have created the resource. See below."'),
      localSecondaryIndex: {
        '#withName':: d.fn(help='"Name of the index"', args=[d.arg(name='name', type=d.T.string)]),
        withName(name): { name: name },
        '#withNonKeyAttributes':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributes(nonKeyAttributes): { nonKeyAttributes: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withNonKeyAttributesMixin':: d.fn(help='"Only required with INCLUDE as a projection type; a list of attributes to project into the index. These do not need to be defined as attributes on the table."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='nonKeyAttributes', type=d.T.array)]),
        withNonKeyAttributesMixin(nonKeyAttributes): { nonKeyAttributes+: if std.isArray(v=nonKeyAttributes) then nonKeyAttributes else [nonKeyAttributes] },
        '#withProjectionType':: d.fn(help='"One of ALL, INCLUDE or KEYS_ONLY where ALL projects every attribute into the index, KEYS_ONLY projects  into the index only the table and index hash_key and sort_key attributes ,  INCLUDE projects into the index all of the attributes that are defined in non_key_attributes in addition to the attributes that thatKEYS_ONLY project."', args=[d.arg(name='projectionType', type=d.T.string)]),
        withProjectionType(projectionType): { projectionType: projectionType },
        '#withRangeKey':: d.fn(help='"Name of the range key."', args=[d.arg(name='rangeKey', type=d.T.string)]),
        withRangeKey(rangeKey): { rangeKey: rangeKey },
      },
      '#onDemandThroughput':: d.obj(help='"Sets the maximum number of read and write units for the specified on-demand table. See below."'),
      onDemandThroughput: {
        '#withMaxReadRequestUnits':: d.fn(help='"Maximum number of read request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxReadRequestUnits', type=d.T.number)]),
        withMaxReadRequestUnits(maxReadRequestUnits): { maxReadRequestUnits: maxReadRequestUnits },
        '#withMaxWriteRequestUnits':: d.fn(help='"Maximum number of write request units for the specified table. To specify set the value greater than or equal to 1. To remove set the value to -1."', args=[d.arg(name='maxWriteRequestUnits', type=d.T.number)]),
        withMaxWriteRequestUnits(maxWriteRequestUnits): { maxWriteRequestUnits: maxWriteRequestUnits },
      },
      '#pointInTimeRecovery':: d.obj(help='"Enable point-in-time recovery options. See below."'),
      pointInTimeRecovery: {
        '#withEnabled':: d.fn(help='"Whether to enable point-in-time recovery. It can take 10 minutes to enable for new tables. If the point_in_time_recovery block is not provided, this defaults to false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
        '#withRecoveryPeriodInDays':: d.fn(help='"Number of preceding days for which continuous backups are taken and maintained. Default is 35."', args=[d.arg(name='recoveryPeriodInDays', type=d.T.number)]),
        withRecoveryPeriodInDays(recoveryPeriodInDays): { recoveryPeriodInDays: recoveryPeriodInDays },
      },
      '#replica':: d.obj(help='"Configuration block(s) with DynamoDB Global Tables V2 (version 2019.11.21) replication configurations. See below."'),
      replica: {
        '#withConsistencyMode':: d.fn(help='"Whether this global table will be using STRONG consistency mode or EVENTUAL consistency mode. Default value is EVENTUAL."', args=[d.arg(name='consistencyMode', type=d.T.string)]),
        withConsistencyMode(consistencyMode): { consistencyMode: consistencyMode },
        '#withDeletionProtectionEnabled':: d.fn(help='"Whether deletion protection is enabled (true) or disabled (false) on the replica. Default is false."', args=[d.arg(name='deletionProtectionEnabled', type=d.T.boolean)]),
        withDeletionProtectionEnabled(deletionProtectionEnabled): { deletionProtectionEnabled: deletionProtectionEnabled },
        '#withKmsKeyArn':: d.fn(help='"ARN of the CMK that should be used for the AWS KMS encryption.\\nThis argument should only be used if the key is different from the default KMS-managed DynamoDB key, alias/aws/dynamodb.\\nNote: This attribute will not be populated with the ARN of default keys.\\nNote: Changing this value will recreate the replica."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
        withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
        '#withPointInTimeRecovery':: d.fn(help='"Whether to enable Point In Time Recovery for the replica. Default is false."', args=[d.arg(name='pointInTimeRecovery', type=d.T.boolean)]),
        withPointInTimeRecovery(pointInTimeRecovery): { pointInTimeRecovery: pointInTimeRecovery },
        '#withPropagateTags':: d.fn(help="\"Whether to propagate the global table's tags to a replica.\\nDefault is false.\\nChanges to tags only move in one direction: from global (source) to replica.\\nTag drift on a replica will not trigger an update.\\nTag changes on the global table are propagated to replicas.\\nChanging from true to false on a subsequent apply leaves replica tags as-is and no longer manages them.\"", args=[d.arg(name='propagateTags', type=d.T.boolean)]),
        withPropagateTags(propagateTags): { propagateTags: propagateTags },
        '#withRegionName':: d.fn(help='"Region name of the replica."', args=[d.arg(name='regionName', type=d.T.string)]),
        withRegionName(regionName): { regionName: regionName },
      },
      '#serverSideEncryption':: d.obj(help="\"Encryption at rest options. AWS DynamoDB tables are automatically encrypted at rest with an AWS-owned Customer Master Key if this argument isn't specified. Must be supplied for cross-region restores. See below.\""),
      serverSideEncryption: {
        '#withEnabled':: d.fn(help='"Whether or not to enable encryption at rest using an AWS managed KMS customer master key (CMK). If enabled is false then server-side encryption is set to AWS-owned key (shown as DEFAULT in the AWS console). Potentially confusingly, if enabled is true and no kms_key_arn is specified then server-side encryption is set to the default KMS-managed key (shown as KMS in the AWS console). The AWS KMS documentation explains the difference between AWS-owned and KMS-managed keys."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
        '#withKmsKeyArn':: d.fn(help='"ARN of the CMK that should be used for the AWS KMS encryption. This argument should only be used if the key is different from the default KMS-managed DynamoDB key, alias/aws/dynamodb. Note: This attribute will not be populated with the ARN of default keys."', args=[d.arg(name='kmsKeyArn', type=d.T.string)]),
        withKmsKeyArn(kmsKeyArn): { kmsKeyArn: kmsKeyArn },
      },
      '#ttl':: d.obj(help='"Configuration block for TTL. See below."'),
      ttl: {
        '#withAttributeName':: d.fn(help='"Name of the table attribute to store the TTL timestamp in.\\nRequired if enabled is true, must not be set otherwise."', args=[d.arg(name='attributeName', type=d.T.string)]),
        withAttributeName(attributeName): { attributeName: attributeName },
        '#withEnabled':: d.fn(help='"Whether TTL is enabled.\\nDefault value is false."', args=[d.arg(name='enabled', type=d.T.boolean)]),
        withEnabled(enabled): { enabled: enabled },
      },
      '#warmThroughput':: d.obj(help='"Sets the number of warm read and write units for the specified table. See below."'),
      warmThroughput: {
        '#withReadUnitsPerSecond':: d.fn(help='"Number of read operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 12000 (default)."', args=[d.arg(name='readUnitsPerSecond', type=d.T.number)]),
        withReadUnitsPerSecond(readUnitsPerSecond): { readUnitsPerSecond: readUnitsPerSecond },
        '#withWriteUnitsPerSecond':: d.fn(help='"Number of write operations a table or index can instantaneously support. For the base table, decreasing this value will force a new resource. For a global secondary index, this value can be increased or decreased without recreation. Minimum value of 4000 (default)."', args=[d.arg(name='writeUnitsPerSecond', type=d.T.number)]),
        withWriteUnitsPerSecond(writeUnitsPerSecond): { writeUnitsPerSecond: writeUnitsPerSecond },
      },
      '#withAttribute':: d.fn(help='"Set of nested attribute definitions. Only required for hash_key and range_key attributes. See below."', args=[d.arg(name='attribute', type=d.T.array)]),
      withAttribute(attribute): { spec+: { initProvider+: { attribute: if std.isArray(v=attribute) then attribute else [attribute] } } },
      '#withAttributeMixin':: d.fn(help='"Set of nested attribute definitions. Only required for hash_key and range_key attributes. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='attribute', type=d.T.array)]),
      withAttributeMixin(attribute): { spec+: { initProvider+: { attribute+: if std.isArray(v=attribute) then attribute else [attribute] } } },
      '#withBillingMode':: d.fn(help='"Controls how you are charged for read and write throughput and how you manage capacity. The valid values are PROVISIONED and PAY_PER_REQUEST. Defaults to PROVISIONED."', args=[d.arg(name='billingMode', type=d.T.string)]),
      withBillingMode(billingMode): { spec+: { initProvider+: { billingMode: billingMode } } },
      '#withDeletionProtectionEnabled':: d.fn(help='"Enables deletion protection for table. Defaults to false."', args=[d.arg(name='deletionProtectionEnabled', type=d.T.boolean)]),
      withDeletionProtectionEnabled(deletionProtectionEnabled): { spec+: { initProvider+: { deletionProtectionEnabled: deletionProtectionEnabled } } },
      '#withGlobalSecondaryIndex':: d.fn(help='"Describe a GSI for the table; subject to the normal limits on the number of GSIs, projected attributes, etc. See below."', args=[d.arg(name='globalSecondaryIndex', type=d.T.array)]),
      withGlobalSecondaryIndex(globalSecondaryIndex): { spec+: { initProvider+: { globalSecondaryIndex: if std.isArray(v=globalSecondaryIndex) then globalSecondaryIndex else [globalSecondaryIndex] } } },
      '#withGlobalSecondaryIndexMixin':: d.fn(help='"Describe a GSI for the table; subject to the normal limits on the number of GSIs, projected attributes, etc. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='globalSecondaryIndex', type=d.T.array)]),
      withGlobalSecondaryIndexMixin(globalSecondaryIndex): { spec+: { initProvider+: { globalSecondaryIndex+: if std.isArray(v=globalSecondaryIndex) then globalSecondaryIndex else [globalSecondaryIndex] } } },
      '#withHashKey':: d.fn(help='"Attribute to use as the hash (partition) key. Must also be defined as an attribute. See below."', args=[d.arg(name='hashKey', type=d.T.string)]),
      withHashKey(hashKey): { spec+: { initProvider+: { hashKey: hashKey } } },
      '#withImportTable':: d.fn(help='"Import Amazon S3 data into a new table. See below."', args=[d.arg(name='importTable', type=d.T.array)]),
      withImportTable(importTable): { spec+: { initProvider+: { importTable: if std.isArray(v=importTable) then importTable else [importTable] } } },
      '#withImportTableMixin':: d.fn(help='"Import Amazon S3 data into a new table. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='importTable', type=d.T.array)]),
      withImportTableMixin(importTable): { spec+: { initProvider+: { importTable+: if std.isArray(v=importTable) then importTable else [importTable] } } },
      '#withLocalSecondaryIndex':: d.fn(help='"Describe an LSI on the table; these can only be allocated at creation so you cannot change this definition after you have created the resource. See below."', args=[d.arg(name='localSecondaryIndex', type=d.T.array)]),
      withLocalSecondaryIndex(localSecondaryIndex): { spec+: { initProvider+: { localSecondaryIndex: if std.isArray(v=localSecondaryIndex) then localSecondaryIndex else [localSecondaryIndex] } } },
      '#withLocalSecondaryIndexMixin':: d.fn(help='"Describe an LSI on the table; these can only be allocated at creation so you cannot change this definition after you have created the resource. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='localSecondaryIndex', type=d.T.array)]),
      withLocalSecondaryIndexMixin(localSecondaryIndex): { spec+: { initProvider+: { localSecondaryIndex+: if std.isArray(v=localSecondaryIndex) then localSecondaryIndex else [localSecondaryIndex] } } },
      '#withOnDemandThroughput':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand table. See below."', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
      withOnDemandThroughput(onDemandThroughput): { spec+: { initProvider+: { onDemandThroughput: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] } } },
      '#withOnDemandThroughputMixin':: d.fn(help='"Sets the maximum number of read and write units for the specified on-demand table. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='onDemandThroughput', type=d.T.array)]),
      withOnDemandThroughputMixin(onDemandThroughput): { spec+: { initProvider+: { onDemandThroughput+: if std.isArray(v=onDemandThroughput) then onDemandThroughput else [onDemandThroughput] } } },
      '#withPointInTimeRecovery':: d.fn(help='"Enable point-in-time recovery options. See below."', args=[d.arg(name='pointInTimeRecovery', type=d.T.array)]),
      withPointInTimeRecovery(pointInTimeRecovery): { spec+: { initProvider+: { pointInTimeRecovery: if std.isArray(v=pointInTimeRecovery) then pointInTimeRecovery else [pointInTimeRecovery] } } },
      '#withPointInTimeRecoveryMixin':: d.fn(help='"Enable point-in-time recovery options. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='pointInTimeRecovery', type=d.T.array)]),
      withPointInTimeRecoveryMixin(pointInTimeRecovery): { spec+: { initProvider+: { pointInTimeRecovery+: if std.isArray(v=pointInTimeRecovery) then pointInTimeRecovery else [pointInTimeRecovery] } } },
      '#withRangeKey':: d.fn(help='"Attribute to use as the range (sort) key. Must also be defined as an attribute, see below."', args=[d.arg(name='rangeKey', type=d.T.string)]),
      withRangeKey(rangeKey): { spec+: { initProvider+: { rangeKey: rangeKey } } },
      '#withReadCapacity':: d.fn(help='"Number of read units for this table. If the billing_mode is PROVISIONED, this field is required."', args=[d.arg(name='readCapacity', type=d.T.number)]),
      withReadCapacity(readCapacity): { spec+: { initProvider+: { readCapacity: readCapacity } } },
      '#withReplica':: d.fn(help='"Configuration block(s) with DynamoDB Global Tables V2 (version 2019.11.21) replication configurations. See below."', args=[d.arg(name='replica', type=d.T.array)]),
      withReplica(replica): { spec+: { initProvider+: { replica: if std.isArray(v=replica) then replica else [replica] } } },
      '#withReplicaMixin':: d.fn(help='"Configuration block(s) with DynamoDB Global Tables V2 (version 2019.11.21) replication configurations. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='replica', type=d.T.array)]),
      withReplicaMixin(replica): { spec+: { initProvider+: { replica+: if std.isArray(v=replica) then replica else [replica] } } },
      '#withRestoreDateTime':: d.fn(help='"Time of the point-in-time recovery point to restore."', args=[d.arg(name='restoreDateTime', type=d.T.string)]),
      withRestoreDateTime(restoreDateTime): { spec+: { initProvider+: { restoreDateTime: restoreDateTime } } },
      '#withRestoreSourceName':: d.fn(help='"Name of the table to restore. Must match the name of an existing table."', args=[d.arg(name='restoreSourceName', type=d.T.string)]),
      withRestoreSourceName(restoreSourceName): { spec+: { initProvider+: { restoreSourceName: restoreSourceName } } },
      '#withRestoreSourceTableArn':: d.fn(help='"ARN of the source table to restore. Must be supplied for cross-region restores."', args=[d.arg(name='restoreSourceTableArn', type=d.T.string)]),
      withRestoreSourceTableArn(restoreSourceTableArn): { spec+: { initProvider+: { restoreSourceTableArn: restoreSourceTableArn } } },
      '#withRestoreToLatestTime':: d.fn(help='"If set, restores table to the most recent point-in-time recovery point."', args=[d.arg(name='restoreToLatestTime', type=d.T.boolean)]),
      withRestoreToLatestTime(restoreToLatestTime): { spec+: { initProvider+: { restoreToLatestTime: restoreToLatestTime } } },
      '#withServerSideEncryption':: d.fn(help="\"Encryption at rest options. AWS DynamoDB tables are automatically encrypted at rest with an AWS-owned Customer Master Key if this argument isn't specified. Must be supplied for cross-region restores. See below.\"", args=[d.arg(name='serverSideEncryption', type=d.T.array)]),
      withServerSideEncryption(serverSideEncryption): { spec+: { initProvider+: { serverSideEncryption: if std.isArray(v=serverSideEncryption) then serverSideEncryption else [serverSideEncryption] } } },
      '#withServerSideEncryptionMixin':: d.fn(help="\"Encryption at rest options. AWS DynamoDB tables are automatically encrypted at rest with an AWS-owned Customer Master Key if this argument isn't specified. Must be supplied for cross-region restores. See below.\"\n\n**Note:** This function appends passed data to existing values", args=[d.arg(name='serverSideEncryption', type=d.T.array)]),
      withServerSideEncryptionMixin(serverSideEncryption): { spec+: { initProvider+: { serverSideEncryption+: if std.isArray(v=serverSideEncryption) then serverSideEncryption else [serverSideEncryption] } } },
      '#withStreamEnabled':: d.fn(help='"Whether Streams are enabled."', args=[d.arg(name='streamEnabled', type=d.T.boolean)]),
      withStreamEnabled(streamEnabled): { spec+: { initProvider+: { streamEnabled: streamEnabled } } },
      '#withStreamViewType':: d.fn(help="\"When an item in the table is modified, StreamViewType determines what information is written to the table's stream. Valid values are KEYS_ONLY, NEW_IMAGE, OLD_IMAGE, NEW_AND_OLD_IMAGES.\"", args=[d.arg(name='streamViewType', type=d.T.string)]),
      withStreamViewType(streamViewType): { spec+: { initProvider+: { streamViewType: streamViewType } } },
      '#withTableClass':: d.fn(help='"Storage class of the table.\\nValid values are STANDARD and STANDARD_INFREQUENT_ACCESS.\\nDefault value is STANDARD."', args=[d.arg(name='tableClass', type=d.T.string)]),
      withTableClass(tableClass): { spec+: { initProvider+: { tableClass: tableClass } } },
      '#withTags':: d.fn(help='"Key-value map of resource tags."', args=[d.arg(name='tags', type=d.T.object)]),
      withTags(tags): { spec+: { initProvider+: { tags: tags } } },
      '#withTagsMixin':: d.fn(help='"Key-value map of resource tags."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='tags', type=d.T.object)]),
      withTagsMixin(tags): { spec+: { initProvider+: { tags+: tags } } },
      '#withTtl':: d.fn(help='"Configuration block for TTL. See below."', args=[d.arg(name='ttl', type=d.T.array)]),
      withTtl(ttl): { spec+: { initProvider+: { ttl: if std.isArray(v=ttl) then ttl else [ttl] } } },
      '#withTtlMixin':: d.fn(help='"Configuration block for TTL. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='ttl', type=d.T.array)]),
      withTtlMixin(ttl): { spec+: { initProvider+: { ttl+: if std.isArray(v=ttl) then ttl else [ttl] } } },
      '#withWarmThroughput':: d.fn(help='"Sets the number of warm read and write units for the specified table. See below."', args=[d.arg(name='warmThroughput', type=d.T.array)]),
      withWarmThroughput(warmThroughput): { spec+: { initProvider+: { warmThroughput: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] } } },
      '#withWarmThroughputMixin':: d.fn(help='"Sets the number of warm read and write units for the specified table. See below."\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='warmThroughput', type=d.T.array)]),
      withWarmThroughputMixin(warmThroughput): { spec+: { initProvider+: { warmThroughput+: if std.isArray(v=warmThroughput) then warmThroughput else [warmThroughput] } } },
      '#withWriteCapacity':: d.fn(help='"Number of write units for this table. If the billing_mode is PROVISIONED, this field is required."', args=[d.arg(name='writeCapacity', type=d.T.number)]),
      withWriteCapacity(writeCapacity): { spec+: { initProvider+: { writeCapacity: writeCapacity } } },
    },
    '#providerConfigRef':: d.obj(help='"ProviderConfigReference specifies how the provider that will be used to\\ncreate, observe, update, and delete this managed resource should be\\nconfigured."'),
    providerConfigRef: {
      '#policy':: d.obj(help='"Policies for referencing."'),
      policy: {
        '#withResolution':: d.fn(help="\"Resolution specifies whether resolution of this reference is required.\\nThe default is 'Required', which means the reconcile will fail if the\\nreference cannot be resolved. 'Optional' means this reference will be\\na no-op if it cannot be resolved.\"", args=[d.arg(name='resolution', type=d.T.string)]),
        withResolution(resolution): { spec+: { providerConfigRef+: { policy+: { resolution: resolution } } } },
        '#withResolve':: d.fn(help="\"Resolve specifies when this reference should be resolved. The default\\nis 'IfNotPresent', which will attempt to resolve the reference only when\\nthe corresponding field is not present. Use 'Always' to resolve the\\nreference on every reconcile.\"", args=[d.arg(name='resolve', type=d.T.string)]),
        withResolve(resolve): { spec+: { providerConfigRef+: { policy+: { resolve: resolve } } } },
      },
      '#withName':: d.fn(help='"Name of the referenced object."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { providerConfigRef+: { name: name } } },
    },
    '#withDeletionPolicy':: d.fn(help='"DeletionPolicy specifies what will happen to the underlying external\\nwhen this managed resource is deleted - either \\"Delete\\" or \\"Orphan\\" the\\nexternal resource.\\nThis field is planned to be deprecated in favor of the ManagementPolicies\\nfield in a future release. Currently, both could be set independently and\\nnon-default values would be honored if the feature flag is enabled.\\nSee the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223"', args=[d.arg(name='deletionPolicy', type=d.T.string)]),
    withDeletionPolicy(deletionPolicy): { spec+: { deletionPolicy: deletionPolicy } },
    '#withManagementPolicies':: d.fn(help='"THIS IS A BETA FIELD. It is on by default but can be opted out\\nthrough a Crossplane feature flag.\\nManagementPolicies specify the array of actions Crossplane is allowed to\\ntake on the managed and external resources.\\nThis field is planned to replace the DeletionPolicy field in a future\\nrelease. Currently, both could be set independently and non-default\\nvalues would be honored if the feature flag is enabled. If both are\\ncustom, the DeletionPolicy field will be ignored.\\nSee the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223\\nand this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md"', args=[d.arg(name='managementPolicies', type=d.T.array)]),
    withManagementPolicies(managementPolicies): { spec+: { managementPolicies: if std.isArray(v=managementPolicies) then managementPolicies else [managementPolicies] } },
    '#withManagementPoliciesMixin':: d.fn(help='"THIS IS A BETA FIELD. It is on by default but can be opted out\\nthrough a Crossplane feature flag.\\nManagementPolicies specify the array of actions Crossplane is allowed to\\ntake on the managed and external resources.\\nThis field is planned to replace the DeletionPolicy field in a future\\nrelease. Currently, both could be set independently and non-default\\nvalues would be honored if the feature flag is enabled. If both are\\ncustom, the DeletionPolicy field will be ignored.\\nSee the design doc for more information: https://github.com/crossplane/crossplane/blob/499895a25d1a1a0ba1604944ef98ac7a1a71f197/design/design-doc-observe-only-resources.md?plain=1#L223\\nand this one: https://github.com/crossplane/crossplane/blob/444267e84783136daa93568b364a5f01228cacbe/design/one-pager-ignore-changes.md"\n\n**Note:** This function appends passed data to existing values', args=[d.arg(name='managementPolicies', type=d.T.array)]),
    withManagementPoliciesMixin(managementPolicies): { spec+: { managementPolicies+: if std.isArray(v=managementPolicies) then managementPolicies else [managementPolicies] } },
    '#writeConnectionSecretToRef':: d.obj(help='"WriteConnectionSecretToReference specifies the namespace and name of a\\nSecret to which any connection details for this managed resource should\\nbe written. Connection details frequently include the endpoint, username,\\nand password required to connect to the managed resource."'),
    writeConnectionSecretToRef: {
      '#withName':: d.fn(help='"Name of the secret."', args=[d.arg(name='name', type=d.T.string)]),
      withName(name): { spec+: { writeConnectionSecretToRef+: { name: name } } },
      '#withNamespace':: d.fn(help='"Namespace of the secret."', args=[d.arg(name='namespace', type=d.T.string)]),
      withNamespace(namespace): { spec+: { writeConnectionSecretToRef+: { namespace: namespace } } },
    },
  },
  '#mixin': 'ignore',
  mixin: self,
}
